{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/python/3.12.1/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /usr/local/python/3.12.1/lib/python3.12/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/codespace/.local/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from nltk) (2024.7.24)\n",
      "Requirement already satisfied: tqdm in /usr/local/python/3.12.1/lib/python3.12/site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: openpyxl in /usr/local/python/3.12.1/lib/python3.12/site-packages (3.1.5)\n",
      "Requirement already satisfied: et-xmlfile in /usr/local/python/3.12.1/lib/python3.12/site-packages (from openpyxl) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/codespace/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_excel('/workspaces/BMIG60303-assignments/All_Articles_Excel_Dec2019July2020.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date Added</th>\n",
       "      <th>Author</th>\n",
       "      <th>Title</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Year</th>\n",
       "      <th>Journal/Publisher</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Issue</th>\n",
       "      <th>Pages</th>\n",
       "      <th>Accession Number</th>\n",
       "      <th>...</th>\n",
       "      <th>Unnamed: 84</th>\n",
       "      <th>Unnamed: 85</th>\n",
       "      <th>Unnamed: 86</th>\n",
       "      <th>Unnamed: 87</th>\n",
       "      <th>Unnamed: 88</th>\n",
       "      <th>Unnamed: 89</th>\n",
       "      <th>Unnamed: 90</th>\n",
       "      <th>Unnamed: 91</th>\n",
       "      <th>Unnamed: 92</th>\n",
       "      <th>Unnamed: 93</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-07-31</td>\n",
       "      <td>Zwerling, A</td>\n",
       "      <td>Understanding spending trends for tuberculosis</td>\n",
       "      <td>Total out-of-pocket spending decreased over th...</td>\n",
       "      <td>2020</td>\n",
       "      <td>Lancet Infectious Diseases</td>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>879-880</td>\n",
       "      <td>2428388309</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-07-31</td>\n",
       "      <td>Zucoloto, GM, Pedro; Porto, Patricia</td>\n",
       "      <td>A propriedade industrial pode limitar o combat...</td>\n",
       "      <td>Esta nota técnica apresenta alguns dos pontos ...</td>\n",
       "      <td>2020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Issue</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-07-31</td>\n",
       "      <td>Zimmer, MAZ, Anne K.; Weißer, Christian W.; Vo...</td>\n",
       "      <td>Hypernatremia—A Manifestation of COVID-19: A C...</td>\n",
       "      <td>We report for the first time therapy-resistant...</td>\n",
       "      <td>2020</td>\n",
       "      <td>A&amp;A Practice</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-07-31</td>\n",
       "      <td>Zignale, M</td>\n",
       "      <td>The experienced space between mobility and Cov...</td>\n",
       "      <td>If it is true that lived space represents our ...</td>\n",
       "      <td>2020</td>\n",
       "      <td>Documenti Geografici</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-07-31</td>\n",
       "      <td>Zhu, XC, Bodong; Avadhanam, Rukmini Manasa; Sh...</td>\n",
       "      <td>Reading and connecting: using social annotatio...</td>\n",
       "      <td>Purpose - The COM-19 pandemic has forced many ...</td>\n",
       "      <td>2020</td>\n",
       "      <td>Information and Learning Sciences</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 94 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Date Added                                             Author  \\\n",
       "0 2020-07-31                                        Zwerling, A   \n",
       "1 2020-07-31               Zucoloto, GM, Pedro; Porto, Patricia   \n",
       "2 2020-07-31  Zimmer, MAZ, Anne K.; Weißer, Christian W.; Vo...   \n",
       "3 2020-07-31                                         Zignale, M   \n",
       "4 2020-07-31  Zhu, XC, Bodong; Avadhanam, Rukmini Manasa; Sh...   \n",
       "\n",
       "                                               Title  \\\n",
       "0     Understanding spending trends for tuberculosis   \n",
       "1  A propriedade industrial pode limitar o combat...   \n",
       "2  Hypernatremia—A Manifestation of COVID-19: A C...   \n",
       "3  The experienced space between mobility and Cov...   \n",
       "4  Reading and connecting: using social annotatio...   \n",
       "\n",
       "                                            Abstract  Year  \\\n",
       "0  Total out-of-pocket spending decreased over th...  2020   \n",
       "1  Esta nota técnica apresenta alguns dos pontos ...  2020   \n",
       "2  We report for the first time therapy-resistant...  2020   \n",
       "3  If it is true that lived space represents our ...  2020   \n",
       "4  Purpose - The COM-19 pandemic has forced many ...  2020   \n",
       "\n",
       "                   Journal/Publisher Volume  Issue    Pages Accession Number  \\\n",
       "0         Lancet Infectious Diseases     20      8  879-880       2428388309   \n",
       "1                                NaN    NaN  Issue      NaN              NaN   \n",
       "2                       A&A Practice    NaN    NaN      NaN              NaN   \n",
       "3               Documenti Geografici    NaN    NaN      NaN              NaN   \n",
       "4  Information and Learning Sciences    NaN    NaN      NaN              NaN   \n",
       "\n",
       "   ... Unnamed: 84 Unnamed: 85 Unnamed: 86 Unnamed: 87 Unnamed: 88  \\\n",
       "0  ...         NaN         NaN         NaN         NaN         NaN   \n",
       "1  ...         NaN         NaN         NaN         NaN         NaN   \n",
       "2  ...         NaN         NaN         NaN         NaN         NaN   \n",
       "3  ...         NaN         NaN         NaN         NaN         NaN   \n",
       "4  ...         NaN         NaN         NaN         NaN         NaN   \n",
       "\n",
       "  Unnamed: 89  Unnamed: 90 Unnamed: 91  Unnamed: 92 Unnamed: 93  \n",
       "0         NaN          NaN         NaN          NaN         NaN  \n",
       "1         NaN          NaN         NaN          NaN         NaN  \n",
       "2         NaN          NaN         NaN          NaN         NaN  \n",
       "3         NaN          NaN         NaN          NaN         NaN  \n",
       "4         NaN          NaN         NaN          NaN         NaN  \n",
       "\n",
       "[5 rows x 94 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Date Added', 'Author', 'Title', 'Abstract', 'Year',\n",
       "       'Journal/Publisher', 'Volume', 'Issue', 'Pages', 'Accession Number',\n",
       "       'DOI', 'URL', 'Name of Database', 'Database Provider', 'Language',\n",
       "       'Keywords', 'Unnamed: 16', 'Unnamed: 17', 'Unnamed: 18', 'Unnamed: 19',\n",
       "       'Unnamed: 20', 'Unnamed: 21', 'Unnamed: 22', 'Unnamed: 23',\n",
       "       'Unnamed: 24', 'Unnamed: 25', 'Unnamed: 26', 'Unnamed: 27',\n",
       "       'Unnamed: 28', 'Unnamed: 29', 'Unnamed: 30', 'Unnamed: 31',\n",
       "       'Unnamed: 32', 'Unnamed: 33', 'Unnamed: 34', 'Unnamed: 35',\n",
       "       'Unnamed: 36', 'Unnamed: 37', 'Unnamed: 38', 'Unnamed: 39',\n",
       "       'Unnamed: 40', 'Unnamed: 41', 'Unnamed: 42', 'Unnamed: 43',\n",
       "       'Unnamed: 44', 'Unnamed: 45', 'Unnamed: 46', 'Unnamed: 47',\n",
       "       'Unnamed: 48', 'Unnamed: 49', 'Unnamed: 50', 'Unnamed: 51',\n",
       "       'Unnamed: 52', 'Unnamed: 53', 'Unnamed: 54', 'Unnamed: 55',\n",
       "       'Unnamed: 56', 'Unnamed: 57', 'Unnamed: 58', 'Unnamed: 59',\n",
       "       'Unnamed: 60', 'Unnamed: 61', 'Unnamed: 62', 'Unnamed: 63',\n",
       "       'Unnamed: 64', 'Unnamed: 65', 'Unnamed: 66', 'Unnamed: 67',\n",
       "       'Unnamed: 68', 'Unnamed: 69', 'Unnamed: 70', 'Unnamed: 71',\n",
       "       'Unnamed: 72', 'Unnamed: 73', 'Unnamed: 74', 'Unnamed: 75',\n",
       "       'Unnamed: 76', 'Unnamed: 77', 'Unnamed: 78', 'Unnamed: 79',\n",
       "       'Unnamed: 80', 'Unnamed: 81', 'Unnamed: 82', 'Unnamed: 83',\n",
       "       'Unnamed: 84', 'Unnamed: 85', 'Unnamed: 86', 'Unnamed: 87',\n",
       "       'Unnamed: 88', 'Unnamed: 89', 'Unnamed: 90', 'Unnamed: 91',\n",
       "       'Unnamed: 92', 'Unnamed: 93'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['Date Added', 'Author', 'Title', 'Abstract', 'Year']]\n",
    "df.dropna(subset=['Abstract'], how='all', inplace=True)\n",
    "df['Abstract'] = df['Abstract'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date Added</th>\n",
       "      <th>Author</th>\n",
       "      <th>Title</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-07-31</td>\n",
       "      <td>Zwerling, A</td>\n",
       "      <td>Understanding spending trends for tuberculosis</td>\n",
       "      <td>total out-of-pocket spending decreased over th...</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-07-31</td>\n",
       "      <td>Zucoloto, GM, Pedro; Porto, Patricia</td>\n",
       "      <td>A propriedade industrial pode limitar o combat...</td>\n",
       "      <td>esta nota técnica apresenta alguns dos pontos ...</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-07-31</td>\n",
       "      <td>Zimmer, MAZ, Anne K.; Weißer, Christian W.; Vo...</td>\n",
       "      <td>Hypernatremia—A Manifestation of COVID-19: A C...</td>\n",
       "      <td>we report for the first time therapy-resistant...</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-07-31</td>\n",
       "      <td>Zignale, M</td>\n",
       "      <td>The experienced space between mobility and Cov...</td>\n",
       "      <td>if it is true that lived space represents our ...</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-07-31</td>\n",
       "      <td>Zhu, XC, Bodong; Avadhanam, Rukmini Manasa; Sh...</td>\n",
       "      <td>Reading and connecting: using social annotatio...</td>\n",
       "      <td>purpose - the com-19 pandemic has forced many ...</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Date Added                                             Author  \\\n",
       "0 2020-07-31                                        Zwerling, A   \n",
       "1 2020-07-31               Zucoloto, GM, Pedro; Porto, Patricia   \n",
       "2 2020-07-31  Zimmer, MAZ, Anne K.; Weißer, Christian W.; Vo...   \n",
       "3 2020-07-31                                         Zignale, M   \n",
       "4 2020-07-31  Zhu, XC, Bodong; Avadhanam, Rukmini Manasa; Sh...   \n",
       "\n",
       "                                               Title  \\\n",
       "0     Understanding spending trends for tuberculosis   \n",
       "1  A propriedade industrial pode limitar o combat...   \n",
       "2  Hypernatremia—A Manifestation of COVID-19: A C...   \n",
       "3  The experienced space between mobility and Cov...   \n",
       "4  Reading and connecting: using social annotatio...   \n",
       "\n",
       "                                            Abstract  Year  \n",
       "0  total out-of-pocket spending decreased over th...  2020  \n",
       "1  esta nota técnica apresenta alguns dos pontos ...  2020  \n",
       "2  we report for the first time therapy-resistant...  2020  \n",
       "3  if it is true that lived space represents our ...  2020  \n",
       "4  purpose - the com-19 pandemic has forced many ...  2020  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"total out-of-pocket spending decreased over the same period; however, although the authors captured direct out-of-pocket spending on medical expenses, they did not include non-medical costs including loss of income, transport, and indirect economic costs due to tuberculosis (many of which are now being collected through who patient cost surveys) in their analysis. the authors' findings show that three countries with strong private sectors—democratic republic of the congo, nigeria, and pakistan—have out-of-pocket medical expenses as the primary source of tuberculosis spending. [...]trends over time and across countries can be used to monitor fluctuations in total tuberculosis spending and assess needs across regions.\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0]['Abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def part01(df):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(df['Abstract'])\n",
    "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "    return tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "np.nan is an invalid document, expected byte or unicode string.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_tfidf \u001b[38;5;241m=\u001b[39m \u001b[43mpart01\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfrac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[24], line 3\u001b[0m, in \u001b[0;36mpart01\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpart01\u001b[39m(df):\n\u001b[1;32m      2\u001b[0m     vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer()\n\u001b[0;32m----> 3\u001b[0m     tfidf_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAbstract\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     tfidf_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(tfidf_matrix\u001b[38;5;241m.\u001b[39mtoarray(), columns\u001b[38;5;241m=\u001b[39mvectorizer\u001b[38;5;241m.\u001b[39mget_feature_names_out())\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tfidf_df\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:2091\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2084\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[1;32m   2085\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[1;32m   2086\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[1;32m   2087\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[1;32m   2088\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[1;32m   2089\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[1;32m   2090\u001b[0m )\n\u001b[0;32m-> 2091\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2092\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[1;32m   2093\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[1;32m   2094\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:1372\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1364\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1365\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1366\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1367\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1368\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1369\u001b[0m             )\n\u001b[1;32m   1370\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1372\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[1;32m   1375\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:1259\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1257\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[1;32m   1258\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m-> 1259\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1260\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1261\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:103\u001b[0m, in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Chain together an optional series of text processing steps to go from\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03ma single document to ngrams, with or without tokenizing or preprocessing.\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    A sequence of tokens, possibly with pairs, triples, etc.\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m decoder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 103\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m analyzer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    105\u001b[0m     doc \u001b[38;5;241m=\u001b[39m analyzer(doc)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:236\u001b[0m, in \u001b[0;36m_VectorizerMixin.decode\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m    233\u001b[0m     doc \u001b[38;5;241m=\u001b[39m doc\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode_error)\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m doc \u001b[38;5;129;01mis\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnan:\n\u001b[0;32m--> 236\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    237\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp.nan is an invalid document, expected byte or unicode string.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    238\u001b[0m     )\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m doc\n",
      "\u001b[0;31mValueError\u001b[0m: np.nan is an invalid document, expected byte or unicode string."
     ]
    }
   ],
   "source": [
    "df_tfidf = part01(df.sample(frac=0.1, random_state=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>0000</th>\n",
       "      <th>00001</th>\n",
       "      <th>000022583</th>\n",
       "      <th>0000471</th>\n",
       "      <th>00006</th>\n",
       "      <th>00007</th>\n",
       "      <th>0001</th>\n",
       "      <th>00015</th>\n",
       "      <th>...</th>\n",
       "      <th>黄芩可通过多途径</th>\n",
       "      <th>黄芩改善covid</th>\n",
       "      <th>黄芩核心靶标</th>\n",
       "      <th>黄芩素</th>\n",
       "      <th>黄芩药对具有抗炎</th>\n",
       "      <th>黄芩药对改善covid</th>\n",
       "      <th>黄芩药对改善新型冠状病毒</th>\n",
       "      <th>黄芩药对的临床应用提供参考</th>\n",
       "      <th>黄芪</th>\n",
       "      <th>鼻塞</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.029039</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40590 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         00  000  0000  00001  000022583  0000471  00006  00007  0001  00015  \\\n",
       "0  0.000000  0.0   0.0    0.0        0.0      0.0    0.0    0.0   0.0    0.0   \n",
       "1  0.029039  0.0   0.0    0.0        0.0      0.0    0.0    0.0   0.0    0.0   \n",
       "2  0.000000  0.0   0.0    0.0        0.0      0.0    0.0    0.0   0.0    0.0   \n",
       "3  0.000000  0.0   0.0    0.0        0.0      0.0    0.0    0.0   0.0    0.0   \n",
       "4  0.000000  0.0   0.0    0.0        0.0      0.0    0.0    0.0   0.0    0.0   \n",
       "\n",
       "   ...  黄芩可通过多途径  黄芩改善covid  黄芩核心靶标  黄芩素  黄芩药对具有抗炎  黄芩药对改善covid  黄芩药对改善新型冠状病毒  \\\n",
       "0  ...       0.0        0.0     0.0  0.0       0.0          0.0           0.0   \n",
       "1  ...       0.0        0.0     0.0  0.0       0.0          0.0           0.0   \n",
       "2  ...       0.0        0.0     0.0  0.0       0.0          0.0           0.0   \n",
       "3  ...       0.0        0.0     0.0  0.0       0.0          0.0           0.0   \n",
       "4  ...       0.0        0.0     0.0  0.0       0.0          0.0           0.0   \n",
       "\n",
       "   黄芩药对的临床应用提供参考   黄芪   鼻塞  \n",
       "0            0.0  0.0  0.0  \n",
       "1            0.0  0.0  0.0  \n",
       "2            0.0  0.0  0.0  \n",
       "3            0.0  0.0  0.0  \n",
       "4            0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 40590 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tfidf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def part02(df_vectors):\n",
    "    top_words_per_abstract = []\n",
    "    for i in range(min(10, df_vectors.shape[0])):\n",
    "        doc_vector = df_vectors.iloc[i]\n",
    "        top_words = doc_vector.sort_values(ascending=False).head(5)\n",
    "        top_words_list = [(word, score) for word, score in top_words.items()]\n",
    "        top_words_per_abstract.append(top_words_list)\n",
    "\n",
    "    return top_words_per_abstract\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = part02(df_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('本文分析武汉某新建治疗新型冠状病毒肺炎医院院所配备的药品目录和四味专有中药组方', 0.3779644730092272),\n",
       "  ('提高临床治疗效果', 0.3779644730092272),\n",
       "  ('分布', 0.3779644730092272),\n",
       "  ('代谢和排泄四个方面总结了中西药相互作用的机制和用药监护要点', 0.3779644730092272),\n",
       "  ('提高用药安全性具有重要意义', 0.3779644730092272)],\n",
       " [('deprivation', 0.41341520534712756),\n",
       "  ('chennai', 0.351177686586386),\n",
       "  ('megacity', 0.30728047576308776),\n",
       "  ('nbr', 0.27614158078264833),\n",
       "  ('wards', 0.2150532548816092)],\n",
       " [('coefficients', 0.2721853810972516),\n",
       "  ('data', 0.2179481927069262),\n",
       "  ('the', 0.1984725447774574),\n",
       "  ('set', 0.17672111264349175),\n",
       "  ('fit', 0.17585399792423398)],\n",
       " [('the', 0.2494100249300536),\n",
       "  ('technology', 0.23192688034845169),\n",
       "  ('synchronicity', 0.21967073876524978),\n",
       "  ('auspicious', 0.21967073876524978),\n",
       "  ('1950s', 0.21967073876524978)],\n",
       " [('同时', 0.2901346225257028),\n",
       "  ('cov', 0.23550810382798631),\n",
       "  ('的来源', 0.15750774213227178),\n",
       "  ('没有证据显示宠物会感染新型冠状病毒', 0.15750774213227178),\n",
       "  ('彰显了宠物与人的亲密程度及其家庭地位之高', 0.15750774213227178)],\n",
       " [('resection', 0.26304733570668354),\n",
       "  ('surgical', 0.22948555741902749),\n",
       "  ('samples', 0.2111692167479466),\n",
       "  ('rna', 0.20863320623670922),\n",
       "  ('handling', 0.20786966297503698)],\n",
       " [('his', 0.5449616996520854),\n",
       "  ('he', 0.3827358558379495),\n",
       "  ('spells', 0.17570885472936712),\n",
       "  ('fevers', 0.15736312475194486),\n",
       "  ('returning', 0.13674835789898201)],\n",
       " [('therapies', 0.4606847334900317),\n",
       "  ('ards', 0.4343105849862237),\n",
       "  ('the', 0.15557648035296243),\n",
       "  ('of', 0.14652222141390758),\n",
       "  ('emerging', 0.1310210282172156)],\n",
       " [('cwf', 0.5895197396275532),\n",
       "  ('fluoridation', 0.35371184377653186),\n",
       "  ('caries', 0.23580789585102124),\n",
       "  ('dental', 0.17137663801472938),\n",
       "  ('led', 0.12491211874832125)],\n",
       " [('recombination', 0.3577245180527117),\n",
       "  ('avian', 0.25847117203546327),\n",
       "  ('covs', 0.23711149816170826),\n",
       "  ('gammacoronavirus', 0.21903096024759477),\n",
       "  ('same', 0.21689134313553535)]]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def part03(df_vectors, query_string):\n",
    "    vectorizer = TfidfVectorizer(vocabulary=df_vectors.columns)\n",
    "    query_vector = vectorizer.fit_transform([query_string])\n",
    "    cosine_similarities = cosine_similarity(query_vector, df_vectors).flatten()\n",
    "    top_5_indices = cosine_similarities.argsort()[-5:][::-1]\n",
    "\n",
    "    return top_5_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_string = \"machine learning\"\n",
    "op_docs = part03(df_tfidf, query_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4106, 4354, 3413, 3113, 4658])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6092    over the past years, several zoonotic viruses ...\n",
       "6420    coronavirus disease 2019, i.e. covid-19, start...\n",
       "5081    two recent lancet and lancet oncology papers r...\n",
       "4625    whether weather plays a part in the transmissi...\n",
       "6737    武 汉 新 型 冠 状 病 毒 肺 炎 是 指 由 新 型 冠 状 病 毒 (2019-nc...\n",
       "Name: Abstract, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[[4106, 4354, 3413, 3113, 4658]]['Abstract']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Italy',\n",
       " 'ITALY',\n",
       " 'ITALIAN',\n",
       " 'Italian',\n",
       " 'italy',\n",
       " 'italian',\n",
       " 'ItaLian',\n",
       " 'iTalY']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = r'\\bItaly|Italian\\b'\n",
    "re.findall(pattern, 'Italy ITALY ITALIAN Italian italy italian ItaLian iTalY', re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_l = []\n",
    "for index, row in df.iterrows():\n",
    "    abstract = row['Abstract']\n",
    "    if isinstance(abstract, str):\n",
    "        matches = re.findall(pattern, abstract, re.IGNORECASE)\n",
    "        match_count = len(matches)\n",
    "        if match_count > 0:\n",
    "            matches_l.append((matches, match_count, abstract))\n",
    "top_matches = sorted(matches_l, key=lambda x: x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Italy', 'Italy', 'Italy', 'Italian', 'Italy', 'Italy', 'Italy', 'Italy', 'Italy', 'Italian', 'Italy', 'Italy']\n",
      "12\n",
      "Italy is currently experiencing an epidemic of COVID-19 which emerged in the Lombardy region . During the interval between February 25-29, 2020, we identified 46 cases of COVID-19 reported in 21 countries in Europe, Africa, North America, and South America which were either in individuals with recent travel from Italy, or who had presumed infection by a traveler from Italy 2. In six cases, in four of the affected countries (Switzerland, France, Austria, Croatia), land travel was a likely route of introduction, or was documented to have been the route of introduction. We used air travel volume between Italian cities and cities in other countries as an index of connectedness, using data available from the International Air Transport Association (IATA) for February 2015 (2.61 million total departing international air passengers from Italy). We used the methods of Fraser et al. to estimate the size of the underlying epidemic in Italy necessary in order for these cases to be observed with a reasonable probability. To estimate the time at risk of COVID-19 exposure for travelers departing Italy, we obtained data from the United Nations World Tourism Organization (UNWTO) for the proportion of international travelers that are non-residents of Italy (63%) and the average length of stay of tourists to Italy (3.4 days), and assumed the Italian epidemic began one month prior to February 29, 2020. We also performed sensitivity analyses in which we included outbound travel to all countries regardless of reported case importations, inflated travel volumes by 35%, to account for the relative increase in flight numbers from 2015-2019, and excluded cases in bordering countries and which were documented to have been introduced by overland travel. When all cases were considered we estimated a true outbreak size of 3971 cases (95% CI 2907-5297), as compared to a reported case count of 1128 on February 29, 2020, suggesting non-identification of 72% (61-79%) of cases. In sensitivity analyses, outbreak sizes varied from 1552 to 4533 cases (implying non-identification of 27-75% of cases). We recently used similar methods to estimate a much larger epidemic size in Iran, with a far greater degree of under-reporting, based on many fewer exported cases. The reason for this difference relates to the relatively high volume of travel from Italy, relative to Iran. In summary, we suggest that the numerous COVID-19 case exportations from Italy in recent days suggest an epidemic that is larger than official case counts suggest, and which is approximately on a par with that currently occurring in South Korea, which reports 3526 cases (and fewer deaths) as of February 29, 2020.Competing Interest StatementThe authors have declared no competing interest.Funding StatementFunded by Canadian Institutes for Health Research COVID-19 Rapid Research Fund.Author DeclarationsAll relevant ethical guidelines have been followed; any necessary IRB and/or ethics committee approvals have been obtained and details of the IRB/oversight body are included in the manuscript.YesAll necessary patient/participant consent has been obtained and the appropriate institutional forms have been archived.YesI understand that all clinical trials and any other prospective interventional studies must be registered with an ICMJE-approved registry, such as ClinicalTrials.gov. I confirm that any such study reported in the manuscript has been registered and the trial registration ID is provided (note: if posting a prospective study registered retrospectively, please provide a statement in the trial ID field explaining why the study was not registered in advance).Yes I have followed all appropriate research reporting guidelines and uploaded the relevant EQUATOR Network research reporting checklist(s) and other pertinent material as supplementary files, if applicable.YesIATA data are proprietary; we are able to provide a table with travel volumes upon request (email david.fisman@utoronto.ca). Other data are publicly available. R code available upon request (email ashleigh.tuite@utoronto.ca).\n",
      "['Italy', 'Italian', 'Italian', 'Italian', 'Italian', 'Italy', 'Italian', 'Italian', 'Italian', 'Italy']\n",
      "10\n",
      "Italy was the first European country to experience sustained local transmission of COVID-19. As of 1st May 2020, the Italian health authorities reported 28,238 deaths nationally. To control the epidemic, the Italian government implemented a suite of non-pharmaceutical interventions (NPIs), including school and university closures, social distancing and full lockdown involving banning of public gatherings and non essential movement. In this report, we model the effect of NPIs on transmission using data on average mobility. We estimate that the average reproduction number (a measure of transmission intensity) is currently below one for all Italian regions, and significantly so for the majority of the regions. Despite the large number of deaths, the proportion of population that has been infected by SARS-CoV-2 (the attack rate) is far from the herd immunity threshold in all Italian regions, with the highest attack rate observed in Lombardy (13.18% [10.66%-16.70%]). Italy is set to relax the currently implemented NPIs from 4th May 2020. Given the control achieved by NPIs, we consider three scenarios for the next 8 weeks: a scenario in which mobility remains the same as during the lockdown, a scenario in which mobility returns to pre-lockdown levels by 20%, and a scenario in which mobility returns to pre-lockdown levels by 40%. The scenarios explored assume that mobility is scaled evenly across all dimensions, that behaviour stays the same as before NPIs were implemented, that no pharmaceutical interventions are introduced, and it does not include transmission reduction from contact tracing, testing and the isolation of confirmed or suspected cases. New interventions, such as enhanced testing and contact tracing are going to be introduced and will likely contribute to reductions in transmission; therefore our estimates should be viewed as pessimistic projections. We find that, in the absence of additional interventions, even a 20% return to pre-lockdown mobility could lead to a resurgence in the number of deaths far greater than experienced in the current wave in several regions. Future increases in the number of deaths will lag behind the increase in transmission intensity and so a second wave will not be immediately apparent from just monitoring of the daily number of deaths. Our results suggest that SARS-CoV-2 transmission as well as mobility should be closely monitored in the next weeks and months. To compensate for the increase in mobility that will occur due to the relaxation of the currently implemented NPIs, adherence to the recommended social distancing measures alongside enhanced community surveillance including swab testing, contact tracing and the early isolation of infections are of paramount importance to reduce the risk of resurgence in transmission.Competing Interest StatementThe authors have declared no competing interest.Funding StatementThis work was supported by Centre funding from the UK Medical Research Council under a concordat with the UK Department for International Development, the NIHR Health Protection Research Unit in Modelling Methodology and Community Jameel.Author DeclarationsAll relevant ethical guidelines have been followed; any necessary IRB and/or ethics committee approvals have been obtained and details of the IRB/oversight body are included in the manuscript.YesAll necessary patient/participant consent has been obtained and the appropriate institutional forms have been archived.YesI understand that all clinical trials and any other prospective interventional studies must be registered with an ICMJE-approved registry, such as ClinicalTrials.gov. I confirm that any such study reported in the manuscript has been registered and the trial registration ID is provided (note: if posting a prospective study registered retrospectively, please provide a statement in the trial ID field explaining why the study was not registered in advance).Yes I have followed all appropriate research reporting guidelines and uploaded the relevant EQUATOR Network research reporting checklist(s) and other pertin nt material as supplementary files, if applicable.YesOur model utilizes daily real-time death data provided by the Italian Civil Protection (publicly available at https://github.com/pcm-dpc/COVID-19) for the 20 Italian regions. For the Trentino Alto-Adige region, we report the results for the provinces of Trento and Bolzano separately, following the format of the death data provided by the Italian Civil Protection. For population counts, we use publicly available age-stratified counts from ISTAT (Popolazione residente comunale per sesso anno di nascita e stato civile, from https://www.istat.it). Mobility data have been obtained from the Google Mobility Report (google.com/covid19/mobility/) which provides data on movement in Italy by regionhttps://github.com/pcm-dpc/COVID-19https://www.istat.it\n",
      "['Italy', 'Italy', 'Italian', 'Italy', 'Italy', 'Italy', 'Italian', 'Italy', 'Italy']\n",
      "9\n",
      "BACKGROUND: The fatality rate of coronavirus disease (COVID-19) in Italy is controversial and is greatly affecting discussion on the impact of containment measures that are straining the world's social and economic fabric, such as instigating large-scale isolation and quarantine, closing borders, imposing limits on public gatherings, and implementing nationwide lockdowns.; OBJECTIVE: The scientific community, citizens, politicians, and mass media are expressing concerns regarding data suggesting that the number of COVID-19-related deaths in Italy is significantly higher than in the rest of the world. Moreover, Italian citizens have misleading perceptions related to the number of swab tests that have actually been performed. Citizens and mass media are denouncing the coverage of COVID-19 swab testing in Italy, claiming that it is not in line with that in other countries worldwide.; METHODS: In this paper, we attempt to clarify the aspects of COVID-19 fatalities and testing in Italy by performing a set of statistical analyses that highlight the actual numbers in Italy and compare them with official worldwide data.; RESULTS: The analysis clearly shows that the Italian COVID-19 fatality and mortality rates are in line with the official world scenario, as are the numbers of COVID-19 tests performed in Italy and in the Lombardy region.; CONCLUSIONS: This up-to-date analysis may elucidate the evolution of the COVID-19 pandemic in Italy.\n",
      "['Italy', 'Italy', 'Italian', 'Italy', 'Italy', 'Italian', 'Italian', 'Italian', 'Italian']\n",
      "9\n",
      "Italy has been one of the countries worst hit by coronavirus disease (COVID-19), with over 185,000 cases and around 80% registered in north of the country [1]. Numbers in the initial phase of the outbreak in Italy seem to suggest a greater severity of the disease, with a higher case fatality rate (CFR) than previously observed in China (7.2% vs 2.3%) [2].The aim of the study was to estimate the excess in total mortality by age and sex during the epidemic in Italian cities.The rapid mortality surveillance system in ItalyGo to section...Since 2004, Italy has had a rapid mortality surveillance system (SiSMG) for real-time monitoring of daily deaths in major Italian cities and allows routine evaluation of the health impact of extreme events and influenza epidemics [3,4]. This surveillance system was a valuable tool for an early evaluation of the direct or indirect impact of COVID-19 on health. It is a standardised surveillance system capable of detecting variations in total mortality in the entire population rather than only on the infected cases, and it does not depend on a specific case definition (i.e. COVID-19-related deaths).Briefly, SiSMG is based on an ad hoc daily flow of mortality data (resident population by age and sex) from local Municipal Registry Offices to the Department of Epidemiology, Lazio Regional Health Authority - ASL Roma 1 (DEPLAZIO) which manages the system on behalf of the Ministry of Health [5,6]. The standardised methodology to evaluate excess mortality typically used in the Italian national surveillance system was considered when estimating the excess related to COVID-19 [3,5-7]. Specifically, the excess was defined as the difference between observed and baseline daily mortality (mean daily value by week and day of the week in the past 5 years). In this report, we show results for a subgroup of 19 cities, representative of almost all Italian regions, with timely updates of data, corresponding to 9 million residents (14% of the Italian population).\n",
      "['Italy', 'Italian', 'Italy', 'Italian', 'Italy', 'Italian', 'Italian', 'Italy', 'Italy']\n",
      "9\n",
      " p Epidemiological figures of the SARS-CoV-2 epidemic in Italy are higher than those observed in China. Our objective was to model the SARS-CoV-2 outbreak progression in Italian regions vs. Lombardy to assess the epidemic’s progression. Our setting was Italy, and especially Lombardy, which is experiencing a heavy burden of SARS-CoV-2 infections. The peak of new daily cases of the epidemic has been reached on the 29th, while was delayed in Central and Southern Italian regions compared to Northern ones. In our models, we estimated the basic reproduction number (R0), which represents the average number of people that can be infected by a person who has already acquired the infection, both by fitting the exponential growth rate of the infection across a 1-month period and also by using day-by-day assessments based on single observations. We used the susceptible–exposed–infected–removed (SEIR) compartment model to predict the spreading of the pandemic in Italy. The two methods provide an agreement of values, although the first method based on exponential fit should provide a better estimation, being computed on the entire time series. Taking into account the growth rate of the infection across a 1-month period, each infected person in Lombardy has involved 4 other people (3.6 based on data of April 23rd) compared to a value of R0 = 2.68, as reported in the Chinese city of Wuhan. According to our model, Piedmont, Veneto, Emilia Romagna, Tuscany and Marche will reach an R0 value of up to 3.5. The R0 was 3.11 for Lazio and 3.14 for the Campania region, where the latter showed the highest value among the Southern Italian regions, followed by Apulia (3.11), Sicily (2.99), Abruzzo (3.0), Calabria (2.84), Basilicata (2.66), and Molise (2.6). The R0 value is decreased in Lombardy and the Northern regions, while it is increased in Central and Southern regions. The expected peak of the SEIR model is set at the end of March, at a national level, with Southern Italian regions reaching the peak in the first days of April. Regarding the strengths and limitations of this study, our model is based on assumptions that might not exactly correspond to the evolution of the epidemic. What we know about the SARS-CoV-2 epidemic is based on Chinese data that seems to be different than those from Italy; Lombardy is experiencing an evolution of the epidemic that seems unique inside Italy and Europe, probably due to demographic and environmental factors. /p \n",
      "['Italian', 'Italian', 'Italy', 'Italian', 'Italian', 'Italy', 'Italian', 'Italian', 'Italy']\n",
      "9\n",
      "The Italian outbreak of COVID-19 cases is a public health emergency and there is a worldwide tremendous interest in the evaluation of the Italian epidemic evolution. Indeed, from February 2020, Italy is facing an explosion of COVID-19 cases. In particular, the Italian observed case fatality rate (CFR) is much higher than the other countries. Recently, it has been hypothesized that the extensive number of intergenerational contacts - typical of Italian culture - could contribute to explain the high number of deaths observed in Italy. However, through an analysis performed for all the Italian regions, here it is shown that the deaths are localized in specific regions and that the CFRs of different Italian regions are overlapping with the rates of European countries. Moreover, through a correlation analyses between CFRs and different social habits, it is shown that no positive correlation is observed between social behaviours and CFRs. In conclusion, this analyses clearly rejects the possibility that social habits and intergenerational contacts can contribute to explain such a profound effect on the number of deaths observed in Italy during COVID-19 outbreak and more effort should be addressed to evaluate the real amount of positive cases. This article is protected by copyright. All rights reserved.\n",
      "['Italy', 'Italian', 'Italian', 'Italian', 'Italian', 'Italian', 'Italy', 'Italian']\n",
      "8\n",
      "Italy was the first Western country to face the COVID-19 pandemic. Here we report the results of a national survey on kidney transplantation activity in February and March 2020, and the results of a three-round Delphi consensus promoted by four scientific societies: the Italian Society of Organ Transplantation, the Italian Society of Nephrology, the Italian Society of Anesthesia and Intensive Care, and the Italian Group on Antimicrobial Stewardship. All 41 Italian transplant centers were invited to express their opinion in the Delphi rounds along with a group of seven experts. The survey revealed that, starting from March 2020, there was a decline in kidney transplantation activity in Italy, especially for living-related transplants. Overall, 60 recipients tested positive for SARS-CoV2 infection, 57 required hospitalization, 17 were admitted to the ICU, and 11 died. The online consensus had high response rates at each round (95.8%, 95.8%, and 89.5%, respectively). Eventually, 27 of 31 proposed statements were approved (87.1%), 12 at the first or second round (38.7%), and 3 at the third (9.7%). Based on the Italian experience, we discuss the reasons for the changes in kidney transplantation activity during the COVID-19 pandemic in Western countries. We also provide working recommendations for the organization and management of kidney transplantation under these conditions.\n",
      "['Italian', 'Italian', 'Italian', 'Italy', 'italian', 'Italy', 'Italy', 'Italy']\n",
      "8\n",
      "Since the spread of Corona Virus Disease 19 (COVID -19), most Italian regions on indication of the Central Government have embarked on a system of either total or partial lock down and have used it as a tool for curbing the spread of COVID-19. This study examines whether lock down can be of help and is it any of the public health policies and can it bring up massive and tremendous change to the health system and general economy to Italian regions that have used it as a way of intervention in the spread of COVID19. The research reviewed literature about word economies with Google as the main search tool; also dealt with to press conferences, editorial reviews from Italian newspapers, Bank of Italy, World Bank, International Monetary Fund and World Health Organization. Interviews were also done through phone calls, questions asked via emails to some of the italian leading epidemiologist, infectious disease specialist in the Italy, carefully reading their scientific works and their citations etc. Also, individual experiences and observations on the COVID-19 pandemic in Italy and measures that policy makers have laid down to mitigate the global health crisis of COVID-19 were also counted. A careful study and analysis of various countries (Germany, Spain, France, Italy) that embarked on lock down either partial or complete is showing plummeting inflation, declining gross domestic product, loss of capital for business groups, loss of jobs especially in the informal sectors, a negative growth due to disruption of the world economy through global value chains, abrupt fall in commodity prices and fiscal revenues and enforcement of travel and social restrictions. The research found that a national lock down is no cure, has never been a cure and is not a cure to any of the pandemics, be it previous or recent either in the history of the Spanish flu, influenza and the pending COVID-19. The research also shows that the 2SQ that is social distancing, self-isolation and quarantine are indispensible tools in this pandemic season and should be enforced to the core to help in the management of COVID-19. In the 2SQ, the 2S stand for social distancing and self-isolation while the Q stands for quarantine.\n",
      "['Italy', 'Italian', 'Italian', 'Italian', 'Italy', 'Italian', 'Italy', 'Italian']\n",
      "8\n",
      "After the initial outbreak in China, the diffusion in Italy of SARS-CoV-2 is exhibiting a clear regional trend with more elevated frequency and severity of cases in Northern areas. Among multiple factors possibly involved in such geographical differences, a role has been hypothesized for atmospheric pollution. We provide additional evidence on the possible influence of air quality, particularly in terms of chronicity of exposure on the spread viral infection in Italian regions. Actual data on Covid-19 outbreak in Italian provinces and corresponding long-term air quality evaluations, were obtained from Italian and European agencies, elaborated and tested for possible interactions. Our elaborations reveal that, beside concentrations, the chronicity of exposure may influence the anomalous variability of SARS-CoV-2 in Italy. Data on distribution of atmospheric pollutants (NO2, O3, PM2.5 and PM10) in Italian regions during the last 4 years, days exceeding regulatory limits, and years of the last decade (2010–2019) in which the limits have been exceeded for at least 35 days, highlight that Northern Italy has been constantly exposed to chronic air pollution. Long-term air-quality data significantly correlated with cases of Covid-19 in up to 71 Italian provinces (updated April 27, 2020) providing further evidence that chronic exposure to atmospheric contamination may represent a favourable context for the spread of the virus. Pro-inflammatory responses and high incidence of respiratory and cardiac affections are well known, while the capability of this coronavirus to bind particulate matters remains to be established. Atmospheric and environmental pollution should be considered as part of an integrated approach for sustainable development, human health protection and prevention of epidemic spreads but in a long-term and chronic perspective, since adoption of mitigation actions during a viral outbreak could be of limited utility.\n",
      "['Italy', 'Italian', 'Italian', 'Italian', 'Italy', 'Italian', 'Italy', 'Italian']\n",
      "8\n",
      "Background: After the initial outbreak in China, the diffusion in Italy of SARS-CoV-2 is exhibiting a clear regional trend with Northern areas being the most affected in terms of both frequency and severity of cases. Among multiple factors possibly involved in such geographical differences, a role has been hypothesized for atmospheric pollution. Objectives: We provide additional evidence on the possible influence of air quality, particularly in terms of chronicity of exposure on the spread viral infection in Italian regions. Methods: Actual data on to COVID-19 outbreak in Italian provinces and corresponding long-term air quality evaluations, were obtained from Italian and European agencies, elaborated and tested for possible interactions. Discussion: Our elaborations reveal that, beside concentrations, the chronicity of exposure may influence the anomalous variability of SARS-CoV-2 in Italy. Data on distribution of atmospheric pollutants (NO2, O3, PM2.5 and PM10) in Italian regions during the last 4 years, days exceeding regulatory limits, and years of the last decade (2010-2019) in which the limits have been exceeded for at least 35 days, confirmed that Northern Italy has been constantly exposed to chronic air pollution. Long-term air-quality data significantly correlated with cases of Covid-19 in up to 71 Italian provinces (updated 6 April) providing further evidence that chronic exposure to atmospheric contamination may represent a favourable context for the spread of the virus. Pro-inflammatory responses and high incidence of respiratory and cardiac affections are well known, while the capability of this coronavirus to bind particulate matters remains to be established. Atmospheric and environmental pollution should be considered as part of an integrated approach for sustainable development, human health protection and prevention of epidemic spreads.Competing Interest StatementThe authors have declared no competing interest.Funding StatementNo external funding was received to elaborate presented dataAuthor DeclarationsAll relevant ethical guidelines have been followed; any necessary IRB and/or ethics committee approvals have been obtained and details of the IRB/oversight body are included in the manuscript.YesAll necessary patient/participant consent has been obtained and the appropriate institutional forms have been archived.YesI understand that all clinical trials and any other prospective interventional studies must be registered with an ICMJE-approved registry, such as ClinicalTrials.gov. I confirm that any such study reported in the manuscript has been registered and the trial registration ID is provided (note: if posting a prospective study registered retrospectively, please provide a statement in the trial ID field explaining why the study was not registered in advance).Yes I have followed all appropriate research reporting guidelines and uploaded the relevant EQUATOR Network research reporting checklist(s) and other pertinent material as supplementary files, if applicable.YesWith the present I certify the availability of all the data used and elaborated in the paper, at the links given below.https://github.com/pcm-dpc/COVID-19https://www.eea.europa.eu/publications/air-quality-in-europe-2019https://www.eea.europa.eu/themes/air/air-quality-and-covid19/monitoring-covid-19-impacts-onhttps://www.legambiente.it/malaria-di-citta/\n"
     ]
    }
   ],
   "source": [
    "for i in top_matches:\n",
    "    print(i[0])\n",
    "    print(i[1])\n",
    "    print(i[2])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2\n",
    "## Test tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'Italy', 'of', 'the', 'ITALY', '.', 'ITALIAN', 'Italian', 'in', 'the', 'realm', '.', 'That', ';', 'right', 'there', 'is', 'italy', '\"', 'italian', '\"', 'ItaLian', 'iTalY', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "text = 'This is a Italy of the ITALY. ITALIAN Italian in the realm. That; right there is italy \"italian\" ItaLian iTalY.'\n",
    "tokens = wordpunct_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = Counter()\n",
    "pattern = re.compile(r'\\b(Italy|Italian)\\b', re.IGNORECASE)\n",
    "for index, row in df.iterrows():\n",
    "    abstract = row['Abstract']\n",
    "    if isinstance(abstract, str):\n",
    "        tokens = wordpunct_tokenize(abstract)\n",
    "        bigrams_list = list(ngrams(tokens, 2))\n",
    "        re_search = [bigram for bigram in bigrams_list if pattern.search(bigram[0])]\n",
    "        bigrams.update(re_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Italy', ',')\t703\n",
      "('Italy', '.')\t282\n",
      "('Italy', 'and')\t274\n",
      "('Italy', 'is')\t94\n",
      "('Italy', '(')\t79\n",
      "('Italian', 'regions')\t77\n",
      "('Italy', 'has')\t69\n",
      "('Italian', 'Society')\t57\n",
      "('Italy', 'was')\t56\n",
      "('Italy', ')')\t54\n",
      "('Italian', 'population')\t34\n",
      "('Italian', 'government')\t33\n",
      "('Italian', 'National')\t28\n",
      "('Italian', 'Ministry')\t28\n",
      "('Italy', '),')\t27\n",
      "('Italy', 'in')\t26\n",
      "('Italian', 'Government')\t23\n",
      "('Italian', 'COVID')\t19\n",
      "('Italy', 'on')\t18\n",
      "('Italy', 'with')\t18\n"
     ]
    }
   ],
   "source": [
    "common_bigrams = bigrams.most_common(20)\n",
    "for i in common_bigrams:\n",
    "    print(str(i[0])+\"\\t\"+ str(i[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3\n",
    "\n",
    "## Test tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Italy',\n",
       " 'ITALY',\n",
       " 'ITALIAN',\n",
       " 'Italian',\n",
       " 'realm',\n",
       " 'right',\n",
       " 'italy',\n",
       " 'italian',\n",
       " 'ItaLian',\n",
       " 'iTalY']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'This is a Italy of the ITALY. ITALIAN Italian in the realm. That; right there is italy \"italian\" ItaLian iTalY.'\n",
    "tokens = wordpunct_tokenize(text)\n",
    "cleaned_tokens = [token for token in tokens if token.isalpha() and token.lower() not in stop_words]\n",
    "cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = Counter()\n",
    "pattern = re.compile(r'\\b(Italy|Italian)\\b', re.IGNORECASE)\n",
    "for index, row in df.iterrows():\n",
    "    abstract = row['Abstract']\n",
    "    if isinstance(abstract, str):\n",
    "        tokens = wordpunct_tokenize(abstract)\n",
    "        cleaned_tokens = [token for token in tokens if token.isalnum() and token.lower() not in stop_words]\n",
    "        bigrams_list = list(ngrams(cleaned_tokens, 2))\n",
    "        re_search = [bigram for bigram in bigrams_list if pattern.search(bigram[0])]\n",
    "        bigrams.update(re_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('Italy', 'Spain'), 177),\n",
       " (('Italian', 'regions'), 77),\n",
       " (('Italian', 'Society'), 57),\n",
       " (('Italy', 'one'), 56),\n",
       " (('Italy', 'first'), 52),\n",
       " (('Italy', 'Iran'), 49),\n",
       " (('Italian', 'population'), 34),\n",
       " (('Italian', 'government'), 33),\n",
       " (('Italy', 'Germany'), 29),\n",
       " (('Italy', 'France'), 29),\n",
       " (('Italian', 'National'), 28),\n",
       " (('Italy', 'China'), 28),\n",
       " (('Italian', 'Ministry'), 28),\n",
       " (('Italy', 'South'), 26),\n",
       " (('Italian', 'Government'), 23),\n",
       " (('Italy', 'USA'), 23),\n",
       " (('Italy', 'March'), 22),\n",
       " (('Italy', 'United'), 20),\n",
       " (('Italian', 'COVID'), 19),\n",
       " (('Italian', 'provinces'), 17)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_bigrams = bigrams.most_common(20)\n",
    "common_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('1557tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kodak Black - Codeine Dreaming &amp;gt;&amp;gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>give me some morphine, is there any more to do?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>just like nicotine,\\nheroin, morphine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Codeine hella calling my name 🙄</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Minus how much coke you youngins be on. Like y...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0            Kodak Black - Codeine Dreaming &gt;&gt;\n",
       "1    give me some morphine, is there any more to do?\n",
       "2              just like nicotine,\\nheroin, morphine\n",
       "3                    Codeine hella calling my name 🙄\n",
       "4  Minus how much coke you youngins be on. Like y..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Synset('gluttonize.v.01'), Synset('sip.v.01'), Synset('receive.v.12'), Synset('pub-crawl.v.01'), Synset('tuck_in.v.01'), Synset('hit.v.15'), Synset('wash_down.v.01'), Synset('ruminate.v.01'), Synset('pop.v.11'), Synset('guzzle.v.01'), Synset('fare.v.02'), Synset('dunk.v.03'), Synset('wine_and_dine.v.01'), Synset('eat_up.v.01'), Synset('sup.v.01'), Synset('eat_out.v.01'), Synset('toss_off.v.02'), Synset('crop.v.05'), Synset('eat.v.01'), Synset('raven.v.02'), Synset('lunch.v.01'), Synset('inject.v.04'), Synset('drop.v.17'), Synset('guggle.v.03'), Synset('eat.v.02'), Synset('tank.v.02'), Synset('use.v.02'), Synset('mess.v.01'), Synset('chain-smoke.v.01'), Synset('port.v.07'), Synset('slurp.v.01'), Synset('nibble.v.03'), Synset('fill_up.v.04'), Synset('mainline.v.01'), Synset('degust.v.01'), Synset('partake.v.03'), Synset('pitch_in.v.01'), Synset('board.v.03'), Synset('break_bread.v.01'), Synset('carry.v.33'), Synset('take_out.v.12'), Synset('swill.v.02'), Synset('picnic.v.01'), Synset('souse.v.03'), Synset('feed.v.06'), Synset('dine.v.01'), Synset('wolf.v.01'), Synset('gobble.v.01'), Synset('inhale.v.01'), Synset('puff.v.01'), Synset('drink.v.02'), Synset('swallow.v.01'), Synset('cannibalize.v.01'), Synset('skin_pop.v.01'), Synset('peck.v.02'), Synset('suck.v.01'), Synset('tipple.v.01'), Synset('claret.v.01'), Synset('lap.v.04'), Synset('victual.v.03'), Synset('devour.v.03'), Synset('browse.v.04'), Synset('devour.v.04'), Synset('forage.v.02'), Synset('wine.v.01'), Synset('cloy.v.02'), Synset('brunch.v.01'), Synset('suckle.v.01'), Synset('bolt.v.03'), Synset('drink.v.01'), Synset('gorge.v.01'), Synset('take_a_hit.v.01'), Synset('free-base.v.01'), Synset('garbage_down.v.01'), Synset('nosh.v.01'), Synset('drain_the_cup.v.01'), Synset('raven.v.04'), Synset('smoke.v.01'), Synset('take_in.v.14'), Synset('gulp.v.01'), Synset('drug.v.02'), Synset('pick_at.v.02'), Synset('sample.v.01'), Synset('feast.v.01'), Synset('drink.v.05'), Synset('satiate.v.01'), Synset('breakfast.v.01'), Synset('dope.v.01'), Synset('eat_in.v.01')}\n",
      "89\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def part01():\n",
    "    synset_consume = wn.synset('consume.v.02')\n",
    "    hyponyms = synset_consume.closure(lambda s: s.hyponyms())\n",
    "    return set(hyponyms)\n",
    "result = part01()\n",
    "print(result)\n",
    "print(len(result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bolt', 'gulp', 'souse', 'tope', 'fress', 'pall', 'guzzle', 'dunk', 'snack', 'whiff', 'pick', 'gurgle', 'inject', 'gobble', 'hit', 'englut', 'guggle', 'feast', 'drop', 'soak', 'dip', 'finish', 'swill', 'partake', 'forage', 'hold', 'stuff', 'victual', 'gormandise', 'breakfast', 'gluttonise', 'use', 'tipple', 'overeat', 'inhale', 'wine', 'gorge', 'dope', 'satiate', 'ruminate', 'port', 'habituate', 'suckle', 'ingurgitate', 'gluttonize', 'gourmandize', 'lunch', 'claret', 'lap', 'dine', 'drink', 'degust', 'cannibalise', 'inebriate', 'feed', 'imbibe', 'smoke', 'cannibalize', 'carry', 'sample', 'sup', 'board', 'graze', 'kill', 'browse', 'prey', 'drug', 'junket', 'nosh', 'swallow', 'slurp', 'piece', 'fuddle', 'pig', 'quaff', 'overindulge', 'brunch', 'suck', 'predate', 'free-base', 'binge', 'base', 'peck', 'eat', 'gormandize', 'banquet', 'chain-smoke', 'sip', 'engorge', 'fare', 'down', 'try', 'receive', 'cloy', 'mess', 'crop', 'picnic', 'glut', 'pub-crawl', 'overgorge', 'wolf', 'snort', 'tank', 'guttle', 'sate', 'mainline', 'pop', 'puff', 'bib', 'swig', 'booze', 'devour', 'fill', 'lick', 'taste', 'range', 'pasture', 'raven', 'replete', 'touch', 'nibble', 'consume'}\n",
      "122\n"
     ]
    }
   ],
   "source": [
    "def part02(synset_set):\n",
    "    lemmas = {lemma for synset in synset_set for lemma in synset.lemma_names()}\n",
    "    filtered_lemmas = {lemma for lemma in lemmas if '_' not in lemma}\n",
    "    return filtered_lemmas\n",
    "\n",
    "synsets = part01()\n",
    "lemmas = part02(synsets)\n",
    "print(lemmas)\n",
    "print(len(lemmas))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.7.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.10-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.9-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.3.0,>=8.2.2 (from spacy)\n",
      "  Downloading thinc-8.2.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.4.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer-0.12.5-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (4.66.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/codespace/.local/lib/python3.12/site-packages (from spacy) (2.32.3)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
      "  Downloading pydantic-2.9.1-py3-none-any.whl.metadata (146 kB)\n",
      "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.12/site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /home/codespace/.local/lib/python3.12/site-packages (from spacy) (73.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.12/site-packages (from spacy) (24.1)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.4.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /home/codespace/.local/lib/python3.12/site-packages (from spacy) (2.1.0)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading language_data-1.2.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.23.3 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading pydantic_core-2.23.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/codespace/.local/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.7.4)\n",
      "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.2.2->spacy)\n",
      "  Downloading blis-0.7.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.3.0,>=8.2.2->spacy)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting numpy>=1.19.0 (from spacy)\n",
      "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich>=10.11.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading rich-13.8.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading cloudpathlib-0.19.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading smart_open-7.0.4-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.12/site-packages (from jinja2->spacy) (2.1.5)\n",
      "Collecting marisa-trie>=0.7.7 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading marisa_trie-1.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.7 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/codespace/.local/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
      "Collecting wrapt (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading wrapt-1.16.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading spacy-3.7.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.7/31.7 MB\u001b[0m \u001b[31m90.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (46 kB)\n",
      "Downloading langcodes-3.4.0-py3-none-any.whl (182 kB)\n",
      "Downloading murmurhash-1.0.10-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n",
      "Downloading preshed-3.0.9-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (156 kB)\n",
      "Downloading pydantic-2.9.1-py3-none-any.whl (434 kB)\n",
      "Downloading pydantic_core-2.23.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.4.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (491 kB)\n",
      "Downloading thinc-8.2.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (865 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.0/865.0 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading typer-0.12.5-py3-none-any.whl (47 kB)\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading blis-0.7.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m85.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cloudpathlib-0.19.0-py3-none-any.whl (49 kB)\n",
      "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading language_data-1.2.0-py3-none-any.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m66.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rich-13.8.1-py3-none-any.whl (241 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading smart_open-7.0.4-py3-none-any.whl (61 kB)\n",
      "Downloading marisa_trie-1.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading wrapt-1.16.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: cymem, wrapt, wasabi, spacy-loggers, spacy-legacy, shellingham, pydantic-core, numpy, murmurhash, mdurl, marisa-trie, cloudpathlib, catalogue, annotated-types, srsly, smart-open, pydantic, preshed, markdown-it-py, language-data, blis, rich, langcodes, confection, typer, thinc, weasel, spacy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.1.0\n",
      "    Uninstalling numpy-2.1.0:\n",
      "      Successfully uninstalled numpy-2.1.0\n",
      "Successfully installed annotated-types-0.7.0 blis-0.7.11 catalogue-2.0.10 cloudpathlib-0.19.0 confection-0.1.5 cymem-2.0.8 langcodes-3.4.0 language-data-1.2.0 marisa-trie-1.2.0 markdown-it-py-3.0.0 mdurl-0.1.2 murmurhash-1.0.10 numpy-1.26.4 preshed-3.0.9 pydantic-2.9.1 pydantic-core-2.23.3 rich-13.8.1 shellingham-1.5.4 smart-open-7.0.4 spacy-3.7.6 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.2.5 typer-0.12.5 wasabi-1.1.3 weasel-0.4.1 wrapt-1.16.0\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stopwords\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Counter\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/spacy/__init__.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Dict, Iterable, Union\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# set library-specific custom warning handling before doing anything else\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m setup_default_warnings\n\u001b[1;32m      8\u001b[0m setup_default_warnings()  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# These are imported as part of the API\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/spacy/errors.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Literal\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mErrorsWithCodes\u001b[39;00m(\u001b[38;5;28mtype\u001b[39m):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, code):\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/spacy/compat.py:39\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcatalogue\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _importlib_metadata \u001b[38;5;28;01mas\u001b[39;00m importlib_metadata  \u001b[38;5;66;03m# type: ignore[no-redef]    # noqa: F401\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthinc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Optimizer  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     41\u001b[0m pickle \u001b[38;5;241m=\u001b[39m pickle\n\u001b[1;32m     42\u001b[0m copy_reg \u001b[38;5;241m=\u001b[39m copy_reg\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/thinc/api.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m     CupyOps,\n\u001b[1;32m      3\u001b[0m     MPSOps,\n\u001b[1;32m      4\u001b[0m     NumpyOps,\n\u001b[1;32m      5\u001b[0m     Ops,\n\u001b[1;32m      6\u001b[0m     get_current_ops,\n\u001b[1;32m      7\u001b[0m     get_ops,\n\u001b[1;32m      8\u001b[0m     set_current_ops,\n\u001b[1;32m      9\u001b[0m     set_gpu_allocator,\n\u001b[1;32m     10\u001b[0m     use_ops,\n\u001b[1;32m     11\u001b[0m     use_pytorch_for_gpu_memory,\n\u001b[1;32m     12\u001b[0m     use_tensorflow_for_gpu_memory,\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m enable_mxnet, enable_tensorflow, has_cupy\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config, ConfigValidationError, registry\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/thinc/backends/__init__.py:17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_cupy_allocators\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cupy_pytorch_allocator, cupy_tensorflow_allocator\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_server\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ParamServer\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcupy_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CupyOps\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmps_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MPSOps\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NumpyOps\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/thinc/backends/cupy_ops.py:16\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      7\u001b[0m     is_cupy_array,\n\u001b[1;32m      8\u001b[0m     is_mxnet_gpu_array,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     torch2xp,\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _custom_kernels\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NumpyOps\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Ops\n\u001b[1;32m     20\u001b[0m \u001b[38;5;129m@registry\u001b[39m\u001b[38;5;241m.\u001b[39mops(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCupyOps\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCupyOps\u001b[39;00m(Ops):\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/thinc/backends/numpy_ops.pyx:1\u001b[0m, in \u001b[0;36minit thinc.backends.numpy_ops\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import string\n",
    "\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "def part03(texts, consume_terms):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    punctuation = set(string.punctuation)\n",
    "    bigram_counter = Counter()\n",
    "    for text in texts:\n",
    "        doc = nlp(text)\n",
    "        tokens = [\n",
    "            token.lemma_.lower() for token in doc\n",
    "            if token.lemma_.lower() not in stop_words and token.lemma_ not in punctuation and not token.is_punct\n",
    "        ]\n",
    "        for i in range(len(tokens) - 1):\n",
    "            first_word = tokens[i]\n",
    "            second_word = tokens[i + 1]\n",
    "            if first_word in consume_terms:\n",
    "                bigram_counter[(first_word, second_word)] += 1\n",
    "    return bigram_counter\n",
    "\n",
    "# Example usage:\n",
    "# Load the tweets from the CSV file\n",
    "df = pd.read_csv('1157tweets.csv')\n",
    "\n",
    "# Get the list of consume terms by calling part01 and part02\n",
    "consume_terms = part02(part01())\n",
    "\n",
    "# Call part03 to process the tweets and find relevant bigrams\n",
    "res = part03(df['text'], consume_terms)\n",
    "\n",
    "# Print the most common bigrams\n",
    "print(res.most_common(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Usage:   \n",
      "  pip uninstall [options] <package> ...\n",
      "  pip uninstall [options] -r <requirements file> ...\n",
      "\n",
      "-r option requires 1 argument\n",
      "\n",
      "Usage:   \n",
      "  pip install [options] <requirement specifier> [package-index-options] ...\n",
      "  pip install [options] -r <requirements file> [package-index-options] ...\n",
      "  pip install [options] [-e] <vcs project url> ...\n",
      "  pip install [options] [-e] <local project path> ...\n",
      "  pip install [options] <archive url/path> ...\n",
      "\n",
      "no such option: --yes\n"
     ]
    }
   ],
   "source": [
    "! --yes -r\n",
    "! --yes -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spacy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 2\u001b[0m     nlp \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcli\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m download\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spacy' is not defined"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except OSError:\n",
    "    from spacy.cli import download\n",
    "    download(\"en_core_web_sm\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/spacy/__init__.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Dict, Iterable, Union\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# set library-specific custom warning handling before doing anything else\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m setup_default_warnings\n\u001b[1;32m      8\u001b[0m setup_default_warnings()  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# These are imported as part of the API\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/spacy/errors.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Literal\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mErrorsWithCodes\u001b[39;00m(\u001b[38;5;28mtype\u001b[39m):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, code):\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/spacy/compat.py:39\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcatalogue\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _importlib_metadata \u001b[38;5;28;01mas\u001b[39;00m importlib_metadata  \u001b[38;5;66;03m# type: ignore[no-redef]    # noqa: F401\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthinc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Optimizer  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     41\u001b[0m pickle \u001b[38;5;241m=\u001b[39m pickle\n\u001b[1;32m     42\u001b[0m copy_reg \u001b[38;5;241m=\u001b[39m copy_reg\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/thinc/api.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m     CupyOps,\n\u001b[1;32m      3\u001b[0m     MPSOps,\n\u001b[1;32m      4\u001b[0m     NumpyOps,\n\u001b[1;32m      5\u001b[0m     Ops,\n\u001b[1;32m      6\u001b[0m     get_current_ops,\n\u001b[1;32m      7\u001b[0m     get_ops,\n\u001b[1;32m      8\u001b[0m     set_current_ops,\n\u001b[1;32m      9\u001b[0m     set_gpu_allocator,\n\u001b[1;32m     10\u001b[0m     use_ops,\n\u001b[1;32m     11\u001b[0m     use_pytorch_for_gpu_memory,\n\u001b[1;32m     12\u001b[0m     use_tensorflow_for_gpu_memory,\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m enable_mxnet, enable_tensorflow, has_cupy\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config, ConfigValidationError, registry\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/thinc/backends/__init__.py:17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_cupy_allocators\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cupy_pytorch_allocator, cupy_tensorflow_allocator\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_server\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ParamServer\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcupy_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CupyOps\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmps_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MPSOps\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NumpyOps\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/thinc/backends/cupy_ops.py:16\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      7\u001b[0m     is_cupy_array,\n\u001b[1;32m      8\u001b[0m     is_mxnet_gpu_array,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     torch2xp,\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _custom_kernels\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NumpyOps\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Ops\n\u001b[1;32m     20\u001b[0m \u001b[38;5;129m@registry\u001b[39m\u001b[38;5;241m.\u001b[39mops(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCupyOps\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCupyOps\u001b[39;00m(Ops):\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/thinc/backends/numpy_ops.pyx:1\u001b[0m, in \u001b[0;36minit thinc.backends.numpy_ops\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

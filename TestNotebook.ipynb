{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/python/3.12.1/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /usr/local/python/3.12.1/lib/python3.12/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/codespace/.local/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from nltk) (2024.7.24)\n",
      "Requirement already satisfied: tqdm in /usr/local/python/3.12.1/lib/python3.12/site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: openpyxl in /usr/local/python/3.12.1/lib/python3.12/site-packages (3.1.5)\n",
      "Requirement already satisfied: et-xmlfile in /usr/local/python/3.12.1/lib/python3.12/site-packages (from openpyxl) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/codespace/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: six in /home/codespace/.local/lib/python3.12/site-packages (from langdetect) (1.16.0)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993222 sha256=bfda66bc5d67d7559cb80e875d7e60d781c2f585607317bfaa6d2e5b741b3734\n",
      "  Stored in directory: /home/codespace/.cache/pip/wheels/c1/67/88/e844b5b022812e15a52e4eaa38a1e709e99f06f6639d7e3ba7\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect\n",
      "Successfully installed langdetect-1.0.9\n"
     ]
    }
   ],
   "source": [
    "!pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDiA\n",
    "from nltk.tokenize import casual_tokenize\n",
    "from langdetect import detect, detect_langs\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_excel('/workspaces/BMIG60303-assignments/All_Articles_Excel_Dec2019July2020.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date Added</th>\n",
       "      <th>Author</th>\n",
       "      <th>Title</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Year</th>\n",
       "      <th>Journal/Publisher</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Issue</th>\n",
       "      <th>Pages</th>\n",
       "      <th>Accession Number</th>\n",
       "      <th>...</th>\n",
       "      <th>Unnamed: 84</th>\n",
       "      <th>Unnamed: 85</th>\n",
       "      <th>Unnamed: 86</th>\n",
       "      <th>Unnamed: 87</th>\n",
       "      <th>Unnamed: 88</th>\n",
       "      <th>Unnamed: 89</th>\n",
       "      <th>Unnamed: 90</th>\n",
       "      <th>Unnamed: 91</th>\n",
       "      <th>Unnamed: 92</th>\n",
       "      <th>Unnamed: 93</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-07-31</td>\n",
       "      <td>Zwerling, A</td>\n",
       "      <td>Understanding spending trends for tuberculosis</td>\n",
       "      <td>Total out-of-pocket spending decreased over th...</td>\n",
       "      <td>2020</td>\n",
       "      <td>Lancet Infectious Diseases</td>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>879-880</td>\n",
       "      <td>2428388309</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-07-31</td>\n",
       "      <td>Zucoloto, GM, Pedro; Porto, Patricia</td>\n",
       "      <td>A propriedade industrial pode limitar o combat...</td>\n",
       "      <td>Esta nota técnica apresenta alguns dos pontos ...</td>\n",
       "      <td>2020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Issue</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-07-31</td>\n",
       "      <td>Zimmer, MAZ, Anne K.; Weißer, Christian W.; Vo...</td>\n",
       "      <td>Hypernatremia—A Manifestation of COVID-19: A C...</td>\n",
       "      <td>We report for the first time therapy-resistant...</td>\n",
       "      <td>2020</td>\n",
       "      <td>A&amp;A Practice</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-07-31</td>\n",
       "      <td>Zignale, M</td>\n",
       "      <td>The experienced space between mobility and Cov...</td>\n",
       "      <td>If it is true that lived space represents our ...</td>\n",
       "      <td>2020</td>\n",
       "      <td>Documenti Geografici</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-07-31</td>\n",
       "      <td>Zhu, XC, Bodong; Avadhanam, Rukmini Manasa; Sh...</td>\n",
       "      <td>Reading and connecting: using social annotatio...</td>\n",
       "      <td>Purpose - The COM-19 pandemic has forced many ...</td>\n",
       "      <td>2020</td>\n",
       "      <td>Information and Learning Sciences</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 94 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Date Added                                             Author  \\\n",
       "0 2020-07-31                                        Zwerling, A   \n",
       "1 2020-07-31               Zucoloto, GM, Pedro; Porto, Patricia   \n",
       "2 2020-07-31  Zimmer, MAZ, Anne K.; Weißer, Christian W.; Vo...   \n",
       "3 2020-07-31                                         Zignale, M   \n",
       "4 2020-07-31  Zhu, XC, Bodong; Avadhanam, Rukmini Manasa; Sh...   \n",
       "\n",
       "                                               Title  \\\n",
       "0     Understanding spending trends for tuberculosis   \n",
       "1  A propriedade industrial pode limitar o combat...   \n",
       "2  Hypernatremia—A Manifestation of COVID-19: A C...   \n",
       "3  The experienced space between mobility and Cov...   \n",
       "4  Reading and connecting: using social annotatio...   \n",
       "\n",
       "                                            Abstract  Year  \\\n",
       "0  Total out-of-pocket spending decreased over th...  2020   \n",
       "1  Esta nota técnica apresenta alguns dos pontos ...  2020   \n",
       "2  We report for the first time therapy-resistant...  2020   \n",
       "3  If it is true that lived space represents our ...  2020   \n",
       "4  Purpose - The COM-19 pandemic has forced many ...  2020   \n",
       "\n",
       "                   Journal/Publisher Volume  Issue    Pages Accession Number  \\\n",
       "0         Lancet Infectious Diseases     20      8  879-880       2428388309   \n",
       "1                                NaN    NaN  Issue      NaN              NaN   \n",
       "2                       A&A Practice    NaN    NaN      NaN              NaN   \n",
       "3               Documenti Geografici    NaN    NaN      NaN              NaN   \n",
       "4  Information and Learning Sciences    NaN    NaN      NaN              NaN   \n",
       "\n",
       "   ... Unnamed: 84 Unnamed: 85 Unnamed: 86 Unnamed: 87 Unnamed: 88  \\\n",
       "0  ...         NaN         NaN         NaN         NaN         NaN   \n",
       "1  ...         NaN         NaN         NaN         NaN         NaN   \n",
       "2  ...         NaN         NaN         NaN         NaN         NaN   \n",
       "3  ...         NaN         NaN         NaN         NaN         NaN   \n",
       "4  ...         NaN         NaN         NaN         NaN         NaN   \n",
       "\n",
       "  Unnamed: 89  Unnamed: 90 Unnamed: 91  Unnamed: 92 Unnamed: 93  \n",
       "0         NaN          NaN         NaN          NaN         NaN  \n",
       "1         NaN          NaN         NaN          NaN         NaN  \n",
       "2         NaN          NaN         NaN          NaN         NaN  \n",
       "3         NaN          NaN         NaN          NaN         NaN  \n",
       "4         NaN          NaN         NaN          NaN         NaN  \n",
       "\n",
       "[5 rows x 94 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Date Added', 'Author', 'Title', 'Abstract', 'Year',\n",
       "       'Journal/Publisher', 'Volume', 'Issue', 'Pages', 'Accession Number',\n",
       "       'DOI', 'URL', 'Name of Database', 'Database Provider', 'Language',\n",
       "       'Keywords', 'Unnamed: 16', 'Unnamed: 17', 'Unnamed: 18', 'Unnamed: 19',\n",
       "       'Unnamed: 20', 'Unnamed: 21', 'Unnamed: 22', 'Unnamed: 23',\n",
       "       'Unnamed: 24', 'Unnamed: 25', 'Unnamed: 26', 'Unnamed: 27',\n",
       "       'Unnamed: 28', 'Unnamed: 29', 'Unnamed: 30', 'Unnamed: 31',\n",
       "       'Unnamed: 32', 'Unnamed: 33', 'Unnamed: 34', 'Unnamed: 35',\n",
       "       'Unnamed: 36', 'Unnamed: 37', 'Unnamed: 38', 'Unnamed: 39',\n",
       "       'Unnamed: 40', 'Unnamed: 41', 'Unnamed: 42', 'Unnamed: 43',\n",
       "       'Unnamed: 44', 'Unnamed: 45', 'Unnamed: 46', 'Unnamed: 47',\n",
       "       'Unnamed: 48', 'Unnamed: 49', 'Unnamed: 50', 'Unnamed: 51',\n",
       "       'Unnamed: 52', 'Unnamed: 53', 'Unnamed: 54', 'Unnamed: 55',\n",
       "       'Unnamed: 56', 'Unnamed: 57', 'Unnamed: 58', 'Unnamed: 59',\n",
       "       'Unnamed: 60', 'Unnamed: 61', 'Unnamed: 62', 'Unnamed: 63',\n",
       "       'Unnamed: 64', 'Unnamed: 65', 'Unnamed: 66', 'Unnamed: 67',\n",
       "       'Unnamed: 68', 'Unnamed: 69', 'Unnamed: 70', 'Unnamed: 71',\n",
       "       'Unnamed: 72', 'Unnamed: 73', 'Unnamed: 74', 'Unnamed: 75',\n",
       "       'Unnamed: 76', 'Unnamed: 77', 'Unnamed: 78', 'Unnamed: 79',\n",
       "       'Unnamed: 80', 'Unnamed: 81', 'Unnamed: 82', 'Unnamed: 83',\n",
       "       'Unnamed: 84', 'Unnamed: 85', 'Unnamed: 86', 'Unnamed: 87',\n",
       "       'Unnamed: 88', 'Unnamed: 89', 'Unnamed: 90', 'Unnamed: 91',\n",
       "       'Unnamed: 92', 'Unnamed: 93'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['Date Added', 'Author', 'Title', 'Abstract', 'Year']]\n",
    "df.dropna(subset=['Abstract'], how='all', inplace=True)\n",
    "df['Abstract'] = df['Abstract'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date Added</th>\n",
       "      <th>Author</th>\n",
       "      <th>Title</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-07-31</td>\n",
       "      <td>Zwerling, A</td>\n",
       "      <td>Understanding spending trends for tuberculosis</td>\n",
       "      <td>total out-of-pocket spending decreased over th...</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-07-31</td>\n",
       "      <td>Zucoloto, GM, Pedro; Porto, Patricia</td>\n",
       "      <td>A propriedade industrial pode limitar o combat...</td>\n",
       "      <td>esta nota técnica apresenta alguns dos pontos ...</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-07-31</td>\n",
       "      <td>Zimmer, MAZ, Anne K.; Weißer, Christian W.; Vo...</td>\n",
       "      <td>Hypernatremia—A Manifestation of COVID-19: A C...</td>\n",
       "      <td>we report for the first time therapy-resistant...</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-07-31</td>\n",
       "      <td>Zignale, M</td>\n",
       "      <td>The experienced space between mobility and Cov...</td>\n",
       "      <td>if it is true that lived space represents our ...</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-07-31</td>\n",
       "      <td>Zhu, XC, Bodong; Avadhanam, Rukmini Manasa; Sh...</td>\n",
       "      <td>Reading and connecting: using social annotatio...</td>\n",
       "      <td>purpose - the com-19 pandemic has forced many ...</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Date Added                                             Author  \\\n",
       "0 2020-07-31                                        Zwerling, A   \n",
       "1 2020-07-31               Zucoloto, GM, Pedro; Porto, Patricia   \n",
       "2 2020-07-31  Zimmer, MAZ, Anne K.; Weißer, Christian W.; Vo...   \n",
       "3 2020-07-31                                         Zignale, M   \n",
       "4 2020-07-31  Zhu, XC, Bodong; Avadhanam, Rukmini Manasa; Sh...   \n",
       "\n",
       "                                               Title  \\\n",
       "0     Understanding spending trends for tuberculosis   \n",
       "1  A propriedade industrial pode limitar o combat...   \n",
       "2  Hypernatremia—A Manifestation of COVID-19: A C...   \n",
       "3  The experienced space between mobility and Cov...   \n",
       "4  Reading and connecting: using social annotatio...   \n",
       "\n",
       "                                            Abstract  Year  \n",
       "0  total out-of-pocket spending decreased over th...  2020  \n",
       "1  esta nota técnica apresenta alguns dos pontos ...  2020  \n",
       "2  we report for the first time therapy-resistant...  2020  \n",
       "3  if it is true that lived space represents our ...  2020  \n",
       "4  purpose - the com-19 pandemic has forced many ...  2020  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"total out-of-pocket spending decreased over the same period; however, although the authors captured direct out-of-pocket spending on medical expenses, they did not include non-medical costs including loss of income, transport, and indirect economic costs due to tuberculosis (many of which are now being collected through who patient cost surveys) in their analysis. the authors' findings show that three countries with strong private sectors—democratic republic of the congo, nigeria, and pakistan—have out-of-pocket medical expenses as the primary source of tuberculosis spending. [...]trends over time and across countries can be used to monitor fluctuations in total tuberculosis spending and assess needs across regions.\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0]['Abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df.Abstract.isna()]\n",
    "\n",
    "\n",
    "def is_english(text, thresh = .9):\n",
    "    try:\n",
    "        langs = detect_langs(text)\n",
    "        return {l.lang : l.prob for l in langs}['en'] > thresh\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "    \n",
    "adf2 = df.sample(5000)\n",
    "adf2['English'] = adf2.Abstract.apply(is_english)\n",
    "adf2 = adf2[adf2.English]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date Added</th>\n",
       "      <th>Author</th>\n",
       "      <th>Title</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Year</th>\n",
       "      <th>English</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>67597</th>\n",
       "      <td>2020-04-07</td>\n",
       "      <td>Gursel, MG, Ihsan</td>\n",
       "      <td>Is Global BCG Vaccination Coverage Relevant To...</td>\n",
       "      <td>the lower than expected number of sars-cov-2 c...</td>\n",
       "      <td>2020</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66011</th>\n",
       "      <td>2020-04-13</td>\n",
       "      <td>Han, YYZ, M. R.; Shi, Y.; Song, Z. H.; Zhou, S...</td>\n",
       "      <td>Application of integrative medicine protocols ...</td>\n",
       "      <td>since the severe acute respiratory syndrome co...</td>\n",
       "      <td>2020</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11294</th>\n",
       "      <td>2020-07-14</td>\n",
       "      <td>Zhu, ST, J.; Gao, H.; He, D.</td>\n",
       "      <td>Age, source, and future risk of COVID-19 infec...</td>\n",
       "      <td>objective: to explore and compare the age, sou...</td>\n",
       "      <td>2020</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28591</th>\n",
       "      <td>2020-06-15</td>\n",
       "      <td>Allali, GM, C.; Grosgurin, O.; Morelot-Panzini...</td>\n",
       "      <td>Dyspnea: the vanished warning symptom of COVID...</td>\n",
       "      <td>since december 2019, the severe acute respirat...</td>\n",
       "      <td>2020</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60306</th>\n",
       "      <td>2020-04-24</td>\n",
       "      <td>Stedman, MD, Mark; Anderson, Simon G.; Lunt, M...</td>\n",
       "      <td>A phased approach to unlocking during the COVI...</td>\n",
       "      <td>with the covid-19 pandemic leading to radical ...</td>\n",
       "      <td>2020</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25174</th>\n",
       "      <td>2020-06-22</td>\n",
       "      <td>Lanjiwar, MNS, Ananthu; Garg, Rakesh; Srivasta...</td>\n",
       "      <td>Neurological Manifestations in Mild SARS-CoV-2...</td>\n",
       "      <td>background: the outbreak of novel corona virus...</td>\n",
       "      <td>2020</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22640</th>\n",
       "      <td>2020-06-24</td>\n",
       "      <td>Zhou, YJZ, K. I.; Wang, X. B.; Sun, Q. F.; Pan...</td>\n",
       "      <td>Metabolic associated fatty liver disease is as...</td>\n",
       "      <td>the corona virus disease 2019 (covid-19) pande...</td>\n",
       "      <td>2020</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75834</th>\n",
       "      <td>2020-03-20</td>\n",
       "      <td>Fang, YN, Y.; Penny, M.</td>\n",
       "      <td>Transmission dynamics of the COVID-19 outbreak...</td>\n",
       "      <td>using the parameterized susceptible-exposed-in...</td>\n",
       "      <td>2020</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25482</th>\n",
       "      <td>2020-06-22</td>\n",
       "      <td>Fosbol, ELB, J. H.; Ostergaard, L.; Andersson,...</td>\n",
       "      <td>Association of Angiotensin-Converting Enzyme I...</td>\n",
       "      <td>importance: it has been hypothesized that angi...</td>\n",
       "      <td>2020</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38090</th>\n",
       "      <td>2020-06-01</td>\n",
       "      <td>Okba, NMAW, I.; van Dieren, B.; Aebischer, A.;...</td>\n",
       "      <td>Particulate multivalent presentation of the re...</td>\n",
       "      <td>middle east respiratory syndrome coronavirus (...</td>\n",
       "      <td>2020</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4478 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Date Added                                             Author  \\\n",
       "67597 2020-04-07                                  Gursel, MG, Ihsan   \n",
       "66011 2020-04-13  Han, YYZ, M. R.; Shi, Y.; Song, Z. H.; Zhou, S...   \n",
       "11294 2020-07-14                       Zhu, ST, J.; Gao, H.; He, D.   \n",
       "28591 2020-06-15  Allali, GM, C.; Grosgurin, O.; Morelot-Panzini...   \n",
       "60306 2020-04-24  Stedman, MD, Mark; Anderson, Simon G.; Lunt, M...   \n",
       "...          ...                                                ...   \n",
       "25174 2020-06-22  Lanjiwar, MNS, Ananthu; Garg, Rakesh; Srivasta...   \n",
       "22640 2020-06-24  Zhou, YJZ, K. I.; Wang, X. B.; Sun, Q. F.; Pan...   \n",
       "75834 2020-03-20                            Fang, YN, Y.; Penny, M.   \n",
       "25482 2020-06-22  Fosbol, ELB, J. H.; Ostergaard, L.; Andersson,...   \n",
       "38090 2020-06-01  Okba, NMAW, I.; van Dieren, B.; Aebischer, A.;...   \n",
       "\n",
       "                                                   Title  \\\n",
       "67597  Is Global BCG Vaccination Coverage Relevant To...   \n",
       "66011  Application of integrative medicine protocols ...   \n",
       "11294  Age, source, and future risk of COVID-19 infec...   \n",
       "28591  Dyspnea: the vanished warning symptom of COVID...   \n",
       "60306  A phased approach to unlocking during the COVI...   \n",
       "...                                                  ...   \n",
       "25174  Neurological Manifestations in Mild SARS-CoV-2...   \n",
       "22640  Metabolic associated fatty liver disease is as...   \n",
       "75834  Transmission dynamics of the COVID-19 outbreak...   \n",
       "25482  Association of Angiotensin-Converting Enzyme I...   \n",
       "38090  Particulate multivalent presentation of the re...   \n",
       "\n",
       "                                                Abstract  Year  English  \n",
       "67597  the lower than expected number of sars-cov-2 c...  2020     True  \n",
       "66011  since the severe acute respiratory syndrome co...  2020     True  \n",
       "11294  objective: to explore and compare the age, sou...  2020     True  \n",
       "28591  since december 2019, the severe acute respirat...  2020     True  \n",
       "60306  with the covid-19 pandemic leading to radical ...  2020     True  \n",
       "...                                                  ...   ...      ...  \n",
       "25174  background: the outbreak of novel corona virus...  2020     True  \n",
       "22640  the corona virus disease 2019 (covid-19) pande...  2020     True  \n",
       "75834  using the parameterized susceptible-exposed-in...  2020     True  \n",
       "25482  importance: it has been hypothesized that angi...  2020     True  \n",
       "38090  middle east respiratory syndrome coronavirus (...  2020     True  \n",
       "\n",
       "[4478 rows x 6 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date Added</th>\n",
       "      <th>Author</th>\n",
       "      <th>Title</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Year</th>\n",
       "      <th>English</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Date Added, Author, Title, Abstract, Year, English]\n",
       "Index: []"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def part01(df):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(df['Abstract'])\n",
    "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "    return tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "np.nan is an invalid document, expected byte or unicode string.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_tfidf \u001b[38;5;241m=\u001b[39m \u001b[43mpart01\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfrac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[24], line 3\u001b[0m, in \u001b[0;36mpart01\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpart01\u001b[39m(df):\n\u001b[1;32m      2\u001b[0m     vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer()\n\u001b[0;32m----> 3\u001b[0m     tfidf_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAbstract\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     tfidf_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(tfidf_matrix\u001b[38;5;241m.\u001b[39mtoarray(), columns\u001b[38;5;241m=\u001b[39mvectorizer\u001b[38;5;241m.\u001b[39mget_feature_names_out())\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tfidf_df\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:2091\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2084\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[1;32m   2085\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[1;32m   2086\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[1;32m   2087\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[1;32m   2088\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[1;32m   2089\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[1;32m   2090\u001b[0m )\n\u001b[0;32m-> 2091\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2092\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[1;32m   2093\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[1;32m   2094\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:1372\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1364\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1365\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1366\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1367\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1368\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1369\u001b[0m             )\n\u001b[1;32m   1370\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1372\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[1;32m   1375\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:1259\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1257\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[1;32m   1258\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m-> 1259\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1260\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1261\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:103\u001b[0m, in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Chain together an optional series of text processing steps to go from\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03ma single document to ngrams, with or without tokenizing or preprocessing.\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    A sequence of tokens, possibly with pairs, triples, etc.\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m decoder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 103\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m analyzer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    105\u001b[0m     doc \u001b[38;5;241m=\u001b[39m analyzer(doc)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:236\u001b[0m, in \u001b[0;36m_VectorizerMixin.decode\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m    233\u001b[0m     doc \u001b[38;5;241m=\u001b[39m doc\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode_error)\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m doc \u001b[38;5;129;01mis\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnan:\n\u001b[0;32m--> 236\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    237\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp.nan is an invalid document, expected byte or unicode string.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    238\u001b[0m     )\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m doc\n",
      "\u001b[0;31mValueError\u001b[0m: np.nan is an invalid document, expected byte or unicode string."
     ]
    }
   ],
   "source": [
    "df_tfidf = part01(df.sample(frac=0.1, random_state=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>0000</th>\n",
       "      <th>00001</th>\n",
       "      <th>000022583</th>\n",
       "      <th>0000471</th>\n",
       "      <th>00006</th>\n",
       "      <th>00007</th>\n",
       "      <th>0001</th>\n",
       "      <th>00015</th>\n",
       "      <th>...</th>\n",
       "      <th>黄芩可通过多途径</th>\n",
       "      <th>黄芩改善covid</th>\n",
       "      <th>黄芩核心靶标</th>\n",
       "      <th>黄芩素</th>\n",
       "      <th>黄芩药对具有抗炎</th>\n",
       "      <th>黄芩药对改善covid</th>\n",
       "      <th>黄芩药对改善新型冠状病毒</th>\n",
       "      <th>黄芩药对的临床应用提供参考</th>\n",
       "      <th>黄芪</th>\n",
       "      <th>鼻塞</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.029039</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40590 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         00  000  0000  00001  000022583  0000471  00006  00007  0001  00015  \\\n",
       "0  0.000000  0.0   0.0    0.0        0.0      0.0    0.0    0.0   0.0    0.0   \n",
       "1  0.029039  0.0   0.0    0.0        0.0      0.0    0.0    0.0   0.0    0.0   \n",
       "2  0.000000  0.0   0.0    0.0        0.0      0.0    0.0    0.0   0.0    0.0   \n",
       "3  0.000000  0.0   0.0    0.0        0.0      0.0    0.0    0.0   0.0    0.0   \n",
       "4  0.000000  0.0   0.0    0.0        0.0      0.0    0.0    0.0   0.0    0.0   \n",
       "\n",
       "   ...  黄芩可通过多途径  黄芩改善covid  黄芩核心靶标  黄芩素  黄芩药对具有抗炎  黄芩药对改善covid  黄芩药对改善新型冠状病毒  \\\n",
       "0  ...       0.0        0.0     0.0  0.0       0.0          0.0           0.0   \n",
       "1  ...       0.0        0.0     0.0  0.0       0.0          0.0           0.0   \n",
       "2  ...       0.0        0.0     0.0  0.0       0.0          0.0           0.0   \n",
       "3  ...       0.0        0.0     0.0  0.0       0.0          0.0           0.0   \n",
       "4  ...       0.0        0.0     0.0  0.0       0.0          0.0           0.0   \n",
       "\n",
       "   黄芩药对的临床应用提供参考   黄芪   鼻塞  \n",
       "0            0.0  0.0  0.0  \n",
       "1            0.0  0.0  0.0  \n",
       "2            0.0  0.0  0.0  \n",
       "3            0.0  0.0  0.0  \n",
       "4            0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 40590 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tfidf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def part02(df_vectors):\n",
    "    top_words_per_abstract = []\n",
    "    for i in range(min(10, df_vectors.shape[0])):\n",
    "        doc_vector = df_vectors.iloc[i]\n",
    "        top_words = doc_vector.sort_values(ascending=False).head(5)\n",
    "        top_words_list = [(word, score) for word, score in top_words.items()]\n",
    "        top_words_per_abstract.append(top_words_list)\n",
    "\n",
    "    return top_words_per_abstract\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = part02(df_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('本文分析武汉某新建治疗新型冠状病毒肺炎医院院所配备的药品目录和四味专有中药组方', 0.3779644730092272),\n",
       "  ('提高临床治疗效果', 0.3779644730092272),\n",
       "  ('分布', 0.3779644730092272),\n",
       "  ('代谢和排泄四个方面总结了中西药相互作用的机制和用药监护要点', 0.3779644730092272),\n",
       "  ('提高用药安全性具有重要意义', 0.3779644730092272)],\n",
       " [('deprivation', 0.41341520534712756),\n",
       "  ('chennai', 0.351177686586386),\n",
       "  ('megacity', 0.30728047576308776),\n",
       "  ('nbr', 0.27614158078264833),\n",
       "  ('wards', 0.2150532548816092)],\n",
       " [('coefficients', 0.2721853810972516),\n",
       "  ('data', 0.2179481927069262),\n",
       "  ('the', 0.1984725447774574),\n",
       "  ('set', 0.17672111264349175),\n",
       "  ('fit', 0.17585399792423398)],\n",
       " [('the', 0.2494100249300536),\n",
       "  ('technology', 0.23192688034845169),\n",
       "  ('synchronicity', 0.21967073876524978),\n",
       "  ('auspicious', 0.21967073876524978),\n",
       "  ('1950s', 0.21967073876524978)],\n",
       " [('同时', 0.2901346225257028),\n",
       "  ('cov', 0.23550810382798631),\n",
       "  ('的来源', 0.15750774213227178),\n",
       "  ('没有证据显示宠物会感染新型冠状病毒', 0.15750774213227178),\n",
       "  ('彰显了宠物与人的亲密程度及其家庭地位之高', 0.15750774213227178)],\n",
       " [('resection', 0.26304733570668354),\n",
       "  ('surgical', 0.22948555741902749),\n",
       "  ('samples', 0.2111692167479466),\n",
       "  ('rna', 0.20863320623670922),\n",
       "  ('handling', 0.20786966297503698)],\n",
       " [('his', 0.5449616996520854),\n",
       "  ('he', 0.3827358558379495),\n",
       "  ('spells', 0.17570885472936712),\n",
       "  ('fevers', 0.15736312475194486),\n",
       "  ('returning', 0.13674835789898201)],\n",
       " [('therapies', 0.4606847334900317),\n",
       "  ('ards', 0.4343105849862237),\n",
       "  ('the', 0.15557648035296243),\n",
       "  ('of', 0.14652222141390758),\n",
       "  ('emerging', 0.1310210282172156)],\n",
       " [('cwf', 0.5895197396275532),\n",
       "  ('fluoridation', 0.35371184377653186),\n",
       "  ('caries', 0.23580789585102124),\n",
       "  ('dental', 0.17137663801472938),\n",
       "  ('led', 0.12491211874832125)],\n",
       " [('recombination', 0.3577245180527117),\n",
       "  ('avian', 0.25847117203546327),\n",
       "  ('covs', 0.23711149816170826),\n",
       "  ('gammacoronavirus', 0.21903096024759477),\n",
       "  ('same', 0.21689134313553535)]]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def part03(df_vectors, query_string):\n",
    "    vectorizer = TfidfVectorizer(vocabulary=df_vectors.columns)\n",
    "    query_vector = vectorizer.fit_transform([query_string])\n",
    "    cosine_similarities = cosine_similarity(query_vector, df_vectors).flatten()\n",
    "    top_5_indices = cosine_similarities.argsort()[-5:][::-1]\n",
    "\n",
    "    return top_5_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_string = \"machine learning\"\n",
    "op_docs = part03(df_tfidf, query_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4106, 4354, 3413, 3113, 4658])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6092    over the past years, several zoonotic viruses ...\n",
       "6420    coronavirus disease 2019, i.e. covid-19, start...\n",
       "5081    two recent lancet and lancet oncology papers r...\n",
       "4625    whether weather plays a part in the transmissi...\n",
       "6737    武 汉 新 型 冠 状 病 毒 肺 炎 是 指 由 新 型 冠 状 病 毒 (2019-nc...\n",
       "Name: Abstract, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[[4106, 4354, 3413, 3113, 4658]]['Abstract']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Italy',\n",
       " 'ITALY',\n",
       " 'ITALIAN',\n",
       " 'Italian',\n",
       " 'italy',\n",
       " 'italian',\n",
       " 'ItaLian',\n",
       " 'iTalY']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = r'\\bItaly|Italian\\b'\n",
    "re.findall(pattern, 'Italy ITALY ITALIAN Italian italy italian ItaLian iTalY', re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_l = []\n",
    "for index, row in df.iterrows():\n",
    "    abstract = row['Abstract']\n",
    "    if isinstance(abstract, str):\n",
    "        matches = re.findall(pattern, abstract, re.IGNORECASE)\n",
    "        match_count = len(matches)\n",
    "        if match_count > 0:\n",
    "            matches_l.append((matches, match_count, abstract))\n",
    "top_matches = sorted(matches_l, key=lambda x: x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Italy', 'Italy', 'Italy', 'Italian', 'Italy', 'Italy', 'Italy', 'Italy', 'Italy', 'Italian', 'Italy', 'Italy']\n",
      "12\n",
      "Italy is currently experiencing an epidemic of COVID-19 which emerged in the Lombardy region . During the interval between February 25-29, 2020, we identified 46 cases of COVID-19 reported in 21 countries in Europe, Africa, North America, and South America which were either in individuals with recent travel from Italy, or who had presumed infection by a traveler from Italy 2. In six cases, in four of the affected countries (Switzerland, France, Austria, Croatia), land travel was a likely route of introduction, or was documented to have been the route of introduction. We used air travel volume between Italian cities and cities in other countries as an index of connectedness, using data available from the International Air Transport Association (IATA) for February 2015 (2.61 million total departing international air passengers from Italy). We used the methods of Fraser et al. to estimate the size of the underlying epidemic in Italy necessary in order for these cases to be observed with a reasonable probability. To estimate the time at risk of COVID-19 exposure for travelers departing Italy, we obtained data from the United Nations World Tourism Organization (UNWTO) for the proportion of international travelers that are non-residents of Italy (63%) and the average length of stay of tourists to Italy (3.4 days), and assumed the Italian epidemic began one month prior to February 29, 2020. We also performed sensitivity analyses in which we included outbound travel to all countries regardless of reported case importations, inflated travel volumes by 35%, to account for the relative increase in flight numbers from 2015-2019, and excluded cases in bordering countries and which were documented to have been introduced by overland travel. When all cases were considered we estimated a true outbreak size of 3971 cases (95% CI 2907-5297), as compared to a reported case count of 1128 on February 29, 2020, suggesting non-identification of 72% (61-79%) of cases. In sensitivity analyses, outbreak sizes varied from 1552 to 4533 cases (implying non-identification of 27-75% of cases). We recently used similar methods to estimate a much larger epidemic size in Iran, with a far greater degree of under-reporting, based on many fewer exported cases. The reason for this difference relates to the relatively high volume of travel from Italy, relative to Iran. In summary, we suggest that the numerous COVID-19 case exportations from Italy in recent days suggest an epidemic that is larger than official case counts suggest, and which is approximately on a par with that currently occurring in South Korea, which reports 3526 cases (and fewer deaths) as of February 29, 2020.Competing Interest StatementThe authors have declared no competing interest.Funding StatementFunded by Canadian Institutes for Health Research COVID-19 Rapid Research Fund.Author DeclarationsAll relevant ethical guidelines have been followed; any necessary IRB and/or ethics committee approvals have been obtained and details of the IRB/oversight body are included in the manuscript.YesAll necessary patient/participant consent has been obtained and the appropriate institutional forms have been archived.YesI understand that all clinical trials and any other prospective interventional studies must be registered with an ICMJE-approved registry, such as ClinicalTrials.gov. I confirm that any such study reported in the manuscript has been registered and the trial registration ID is provided (note: if posting a prospective study registered retrospectively, please provide a statement in the trial ID field explaining why the study was not registered in advance).Yes I have followed all appropriate research reporting guidelines and uploaded the relevant EQUATOR Network research reporting checklist(s) and other pertinent material as supplementary files, if applicable.YesIATA data are proprietary; we are able to provide a table with travel volumes upon request (email david.fisman@utoronto.ca). Other data are publicly available. R code available upon request (email ashleigh.tuite@utoronto.ca).\n",
      "['Italy', 'Italian', 'Italian', 'Italian', 'Italian', 'Italy', 'Italian', 'Italian', 'Italian', 'Italy']\n",
      "10\n",
      "Italy was the first European country to experience sustained local transmission of COVID-19. As of 1st May 2020, the Italian health authorities reported 28,238 deaths nationally. To control the epidemic, the Italian government implemented a suite of non-pharmaceutical interventions (NPIs), including school and university closures, social distancing and full lockdown involving banning of public gatherings and non essential movement. In this report, we model the effect of NPIs on transmission using data on average mobility. We estimate that the average reproduction number (a measure of transmission intensity) is currently below one for all Italian regions, and significantly so for the majority of the regions. Despite the large number of deaths, the proportion of population that has been infected by SARS-CoV-2 (the attack rate) is far from the herd immunity threshold in all Italian regions, with the highest attack rate observed in Lombardy (13.18% [10.66%-16.70%]). Italy is set to relax the currently implemented NPIs from 4th May 2020. Given the control achieved by NPIs, we consider three scenarios for the next 8 weeks: a scenario in which mobility remains the same as during the lockdown, a scenario in which mobility returns to pre-lockdown levels by 20%, and a scenario in which mobility returns to pre-lockdown levels by 40%. The scenarios explored assume that mobility is scaled evenly across all dimensions, that behaviour stays the same as before NPIs were implemented, that no pharmaceutical interventions are introduced, and it does not include transmission reduction from contact tracing, testing and the isolation of confirmed or suspected cases. New interventions, such as enhanced testing and contact tracing are going to be introduced and will likely contribute to reductions in transmission; therefore our estimates should be viewed as pessimistic projections. We find that, in the absence of additional interventions, even a 20% return to pre-lockdown mobility could lead to a resurgence in the number of deaths far greater than experienced in the current wave in several regions. Future increases in the number of deaths will lag behind the increase in transmission intensity and so a second wave will not be immediately apparent from just monitoring of the daily number of deaths. Our results suggest that SARS-CoV-2 transmission as well as mobility should be closely monitored in the next weeks and months. To compensate for the increase in mobility that will occur due to the relaxation of the currently implemented NPIs, adherence to the recommended social distancing measures alongside enhanced community surveillance including swab testing, contact tracing and the early isolation of infections are of paramount importance to reduce the risk of resurgence in transmission.Competing Interest StatementThe authors have declared no competing interest.Funding StatementThis work was supported by Centre funding from the UK Medical Research Council under a concordat with the UK Department for International Development, the NIHR Health Protection Research Unit in Modelling Methodology and Community Jameel.Author DeclarationsAll relevant ethical guidelines have been followed; any necessary IRB and/or ethics committee approvals have been obtained and details of the IRB/oversight body are included in the manuscript.YesAll necessary patient/participant consent has been obtained and the appropriate institutional forms have been archived.YesI understand that all clinical trials and any other prospective interventional studies must be registered with an ICMJE-approved registry, such as ClinicalTrials.gov. I confirm that any such study reported in the manuscript has been registered and the trial registration ID is provided (note: if posting a prospective study registered retrospectively, please provide a statement in the trial ID field explaining why the study was not registered in advance).Yes I have followed all appropriate research reporting guidelines and uploaded the relevant EQUATOR Network research reporting checklist(s) and other pertin nt material as supplementary files, if applicable.YesOur model utilizes daily real-time death data provided by the Italian Civil Protection (publicly available at https://github.com/pcm-dpc/COVID-19) for the 20 Italian regions. For the Trentino Alto-Adige region, we report the results for the provinces of Trento and Bolzano separately, following the format of the death data provided by the Italian Civil Protection. For population counts, we use publicly available age-stratified counts from ISTAT (Popolazione residente comunale per sesso anno di nascita e stato civile, from https://www.istat.it). Mobility data have been obtained from the Google Mobility Report (google.com/covid19/mobility/) which provides data on movement in Italy by regionhttps://github.com/pcm-dpc/COVID-19https://www.istat.it\n",
      "['Italy', 'Italy', 'Italian', 'Italy', 'Italy', 'Italy', 'Italian', 'Italy', 'Italy']\n",
      "9\n",
      "BACKGROUND: The fatality rate of coronavirus disease (COVID-19) in Italy is controversial and is greatly affecting discussion on the impact of containment measures that are straining the world's social and economic fabric, such as instigating large-scale isolation and quarantine, closing borders, imposing limits on public gatherings, and implementing nationwide lockdowns.; OBJECTIVE: The scientific community, citizens, politicians, and mass media are expressing concerns regarding data suggesting that the number of COVID-19-related deaths in Italy is significantly higher than in the rest of the world. Moreover, Italian citizens have misleading perceptions related to the number of swab tests that have actually been performed. Citizens and mass media are denouncing the coverage of COVID-19 swab testing in Italy, claiming that it is not in line with that in other countries worldwide.; METHODS: In this paper, we attempt to clarify the aspects of COVID-19 fatalities and testing in Italy by performing a set of statistical analyses that highlight the actual numbers in Italy and compare them with official worldwide data.; RESULTS: The analysis clearly shows that the Italian COVID-19 fatality and mortality rates are in line with the official world scenario, as are the numbers of COVID-19 tests performed in Italy and in the Lombardy region.; CONCLUSIONS: This up-to-date analysis may elucidate the evolution of the COVID-19 pandemic in Italy.\n",
      "['Italy', 'Italy', 'Italian', 'Italy', 'Italy', 'Italian', 'Italian', 'Italian', 'Italian']\n",
      "9\n",
      "Italy has been one of the countries worst hit by coronavirus disease (COVID-19), with over 185,000 cases and around 80% registered in north of the country [1]. Numbers in the initial phase of the outbreak in Italy seem to suggest a greater severity of the disease, with a higher case fatality rate (CFR) than previously observed in China (7.2% vs 2.3%) [2].The aim of the study was to estimate the excess in total mortality by age and sex during the epidemic in Italian cities.The rapid mortality surveillance system in ItalyGo to section...Since 2004, Italy has had a rapid mortality surveillance system (SiSMG) for real-time monitoring of daily deaths in major Italian cities and allows routine evaluation of the health impact of extreme events and influenza epidemics [3,4]. This surveillance system was a valuable tool for an early evaluation of the direct or indirect impact of COVID-19 on health. It is a standardised surveillance system capable of detecting variations in total mortality in the entire population rather than only on the infected cases, and it does not depend on a specific case definition (i.e. COVID-19-related deaths).Briefly, SiSMG is based on an ad hoc daily flow of mortality data (resident population by age and sex) from local Municipal Registry Offices to the Department of Epidemiology, Lazio Regional Health Authority - ASL Roma 1 (DEPLAZIO) which manages the system on behalf of the Ministry of Health [5,6]. The standardised methodology to evaluate excess mortality typically used in the Italian national surveillance system was considered when estimating the excess related to COVID-19 [3,5-7]. Specifically, the excess was defined as the difference between observed and baseline daily mortality (mean daily value by week and day of the week in the past 5 years). In this report, we show results for a subgroup of 19 cities, representative of almost all Italian regions, with timely updates of data, corresponding to 9 million residents (14% of the Italian population).\n",
      "['Italy', 'Italian', 'Italy', 'Italian', 'Italy', 'Italian', 'Italian', 'Italy', 'Italy']\n",
      "9\n",
      " p Epidemiological figures of the SARS-CoV-2 epidemic in Italy are higher than those observed in China. Our objective was to model the SARS-CoV-2 outbreak progression in Italian regions vs. Lombardy to assess the epidemic’s progression. Our setting was Italy, and especially Lombardy, which is experiencing a heavy burden of SARS-CoV-2 infections. The peak of new daily cases of the epidemic has been reached on the 29th, while was delayed in Central and Southern Italian regions compared to Northern ones. In our models, we estimated the basic reproduction number (R0), which represents the average number of people that can be infected by a person who has already acquired the infection, both by fitting the exponential growth rate of the infection across a 1-month period and also by using day-by-day assessments based on single observations. We used the susceptible–exposed–infected–removed (SEIR) compartment model to predict the spreading of the pandemic in Italy. The two methods provide an agreement of values, although the first method based on exponential fit should provide a better estimation, being computed on the entire time series. Taking into account the growth rate of the infection across a 1-month period, each infected person in Lombardy has involved 4 other people (3.6 based on data of April 23rd) compared to a value of R0 = 2.68, as reported in the Chinese city of Wuhan. According to our model, Piedmont, Veneto, Emilia Romagna, Tuscany and Marche will reach an R0 value of up to 3.5. The R0 was 3.11 for Lazio and 3.14 for the Campania region, where the latter showed the highest value among the Southern Italian regions, followed by Apulia (3.11), Sicily (2.99), Abruzzo (3.0), Calabria (2.84), Basilicata (2.66), and Molise (2.6). The R0 value is decreased in Lombardy and the Northern regions, while it is increased in Central and Southern regions. The expected peak of the SEIR model is set at the end of March, at a national level, with Southern Italian regions reaching the peak in the first days of April. Regarding the strengths and limitations of this study, our model is based on assumptions that might not exactly correspond to the evolution of the epidemic. What we know about the SARS-CoV-2 epidemic is based on Chinese data that seems to be different than those from Italy; Lombardy is experiencing an evolution of the epidemic that seems unique inside Italy and Europe, probably due to demographic and environmental factors. /p \n",
      "['Italian', 'Italian', 'Italy', 'Italian', 'Italian', 'Italy', 'Italian', 'Italian', 'Italy']\n",
      "9\n",
      "The Italian outbreak of COVID-19 cases is a public health emergency and there is a worldwide tremendous interest in the evaluation of the Italian epidemic evolution. Indeed, from February 2020, Italy is facing an explosion of COVID-19 cases. In particular, the Italian observed case fatality rate (CFR) is much higher than the other countries. Recently, it has been hypothesized that the extensive number of intergenerational contacts - typical of Italian culture - could contribute to explain the high number of deaths observed in Italy. However, through an analysis performed for all the Italian regions, here it is shown that the deaths are localized in specific regions and that the CFRs of different Italian regions are overlapping with the rates of European countries. Moreover, through a correlation analyses between CFRs and different social habits, it is shown that no positive correlation is observed between social behaviours and CFRs. In conclusion, this analyses clearly rejects the possibility that social habits and intergenerational contacts can contribute to explain such a profound effect on the number of deaths observed in Italy during COVID-19 outbreak and more effort should be addressed to evaluate the real amount of positive cases. This article is protected by copyright. All rights reserved.\n",
      "['Italy', 'Italian', 'Italian', 'Italian', 'Italian', 'Italian', 'Italy', 'Italian']\n",
      "8\n",
      "Italy was the first Western country to face the COVID-19 pandemic. Here we report the results of a national survey on kidney transplantation activity in February and March 2020, and the results of a three-round Delphi consensus promoted by four scientific societies: the Italian Society of Organ Transplantation, the Italian Society of Nephrology, the Italian Society of Anesthesia and Intensive Care, and the Italian Group on Antimicrobial Stewardship. All 41 Italian transplant centers were invited to express their opinion in the Delphi rounds along with a group of seven experts. The survey revealed that, starting from March 2020, there was a decline in kidney transplantation activity in Italy, especially for living-related transplants. Overall, 60 recipients tested positive for SARS-CoV2 infection, 57 required hospitalization, 17 were admitted to the ICU, and 11 died. The online consensus had high response rates at each round (95.8%, 95.8%, and 89.5%, respectively). Eventually, 27 of 31 proposed statements were approved (87.1%), 12 at the first or second round (38.7%), and 3 at the third (9.7%). Based on the Italian experience, we discuss the reasons for the changes in kidney transplantation activity during the COVID-19 pandemic in Western countries. We also provide working recommendations for the organization and management of kidney transplantation under these conditions.\n",
      "['Italian', 'Italian', 'Italian', 'Italy', 'italian', 'Italy', 'Italy', 'Italy']\n",
      "8\n",
      "Since the spread of Corona Virus Disease 19 (COVID -19), most Italian regions on indication of the Central Government have embarked on a system of either total or partial lock down and have used it as a tool for curbing the spread of COVID-19. This study examines whether lock down can be of help and is it any of the public health policies and can it bring up massive and tremendous change to the health system and general economy to Italian regions that have used it as a way of intervention in the spread of COVID19. The research reviewed literature about word economies with Google as the main search tool; also dealt with to press conferences, editorial reviews from Italian newspapers, Bank of Italy, World Bank, International Monetary Fund and World Health Organization. Interviews were also done through phone calls, questions asked via emails to some of the italian leading epidemiologist, infectious disease specialist in the Italy, carefully reading their scientific works and their citations etc. Also, individual experiences and observations on the COVID-19 pandemic in Italy and measures that policy makers have laid down to mitigate the global health crisis of COVID-19 were also counted. A careful study and analysis of various countries (Germany, Spain, France, Italy) that embarked on lock down either partial or complete is showing plummeting inflation, declining gross domestic product, loss of capital for business groups, loss of jobs especially in the informal sectors, a negative growth due to disruption of the world economy through global value chains, abrupt fall in commodity prices and fiscal revenues and enforcement of travel and social restrictions. The research found that a national lock down is no cure, has never been a cure and is not a cure to any of the pandemics, be it previous or recent either in the history of the Spanish flu, influenza and the pending COVID-19. The research also shows that the 2SQ that is social distancing, self-isolation and quarantine are indispensible tools in this pandemic season and should be enforced to the core to help in the management of COVID-19. In the 2SQ, the 2S stand for social distancing and self-isolation while the Q stands for quarantine.\n",
      "['Italy', 'Italian', 'Italian', 'Italian', 'Italy', 'Italian', 'Italy', 'Italian']\n",
      "8\n",
      "After the initial outbreak in China, the diffusion in Italy of SARS-CoV-2 is exhibiting a clear regional trend with more elevated frequency and severity of cases in Northern areas. Among multiple factors possibly involved in such geographical differences, a role has been hypothesized for atmospheric pollution. We provide additional evidence on the possible influence of air quality, particularly in terms of chronicity of exposure on the spread viral infection in Italian regions. Actual data on Covid-19 outbreak in Italian provinces and corresponding long-term air quality evaluations, were obtained from Italian and European agencies, elaborated and tested for possible interactions. Our elaborations reveal that, beside concentrations, the chronicity of exposure may influence the anomalous variability of SARS-CoV-2 in Italy. Data on distribution of atmospheric pollutants (NO2, O3, PM2.5 and PM10) in Italian regions during the last 4 years, days exceeding regulatory limits, and years of the last decade (2010–2019) in which the limits have been exceeded for at least 35 days, highlight that Northern Italy has been constantly exposed to chronic air pollution. Long-term air-quality data significantly correlated with cases of Covid-19 in up to 71 Italian provinces (updated April 27, 2020) providing further evidence that chronic exposure to atmospheric contamination may represent a favourable context for the spread of the virus. Pro-inflammatory responses and high incidence of respiratory and cardiac affections are well known, while the capability of this coronavirus to bind particulate matters remains to be established. Atmospheric and environmental pollution should be considered as part of an integrated approach for sustainable development, human health protection and prevention of epidemic spreads but in a long-term and chronic perspective, since adoption of mitigation actions during a viral outbreak could be of limited utility.\n",
      "['Italy', 'Italian', 'Italian', 'Italian', 'Italy', 'Italian', 'Italy', 'Italian']\n",
      "8\n",
      "Background: After the initial outbreak in China, the diffusion in Italy of SARS-CoV-2 is exhibiting a clear regional trend with Northern areas being the most affected in terms of both frequency and severity of cases. Among multiple factors possibly involved in such geographical differences, a role has been hypothesized for atmospheric pollution. Objectives: We provide additional evidence on the possible influence of air quality, particularly in terms of chronicity of exposure on the spread viral infection in Italian regions. Methods: Actual data on to COVID-19 outbreak in Italian provinces and corresponding long-term air quality evaluations, were obtained from Italian and European agencies, elaborated and tested for possible interactions. Discussion: Our elaborations reveal that, beside concentrations, the chronicity of exposure may influence the anomalous variability of SARS-CoV-2 in Italy. Data on distribution of atmospheric pollutants (NO2, O3, PM2.5 and PM10) in Italian regions during the last 4 years, days exceeding regulatory limits, and years of the last decade (2010-2019) in which the limits have been exceeded for at least 35 days, confirmed that Northern Italy has been constantly exposed to chronic air pollution. Long-term air-quality data significantly correlated with cases of Covid-19 in up to 71 Italian provinces (updated 6 April) providing further evidence that chronic exposure to atmospheric contamination may represent a favourable context for the spread of the virus. Pro-inflammatory responses and high incidence of respiratory and cardiac affections are well known, while the capability of this coronavirus to bind particulate matters remains to be established. Atmospheric and environmental pollution should be considered as part of an integrated approach for sustainable development, human health protection and prevention of epidemic spreads.Competing Interest StatementThe authors have declared no competing interest.Funding StatementNo external funding was received to elaborate presented dataAuthor DeclarationsAll relevant ethical guidelines have been followed; any necessary IRB and/or ethics committee approvals have been obtained and details of the IRB/oversight body are included in the manuscript.YesAll necessary patient/participant consent has been obtained and the appropriate institutional forms have been archived.YesI understand that all clinical trials and any other prospective interventional studies must be registered with an ICMJE-approved registry, such as ClinicalTrials.gov. I confirm that any such study reported in the manuscript has been registered and the trial registration ID is provided (note: if posting a prospective study registered retrospectively, please provide a statement in the trial ID field explaining why the study was not registered in advance).Yes I have followed all appropriate research reporting guidelines and uploaded the relevant EQUATOR Network research reporting checklist(s) and other pertinent material as supplementary files, if applicable.YesWith the present I certify the availability of all the data used and elaborated in the paper, at the links given below.https://github.com/pcm-dpc/COVID-19https://www.eea.europa.eu/publications/air-quality-in-europe-2019https://www.eea.europa.eu/themes/air/air-quality-and-covid19/monitoring-covid-19-impacts-onhttps://www.legambiente.it/malaria-di-citta/\n"
     ]
    }
   ],
   "source": [
    "for i in top_matches:\n",
    "    print(i[0])\n",
    "    print(i[1])\n",
    "    print(i[2])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2\n",
    "## Test tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'Italy', 'of', 'the', 'ITALY', '.', 'ITALIAN', 'Italian', 'in', 'the', 'realm', '.', 'That', ';', 'right', 'there', 'is', 'italy', '\"', 'italian', '\"', 'ItaLian', 'iTalY', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "text = 'This is a Italy of the ITALY. ITALIAN Italian in the realm. That; right there is italy \"italian\" ItaLian iTalY.'\n",
    "tokens = wordpunct_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = Counter()\n",
    "pattern = re.compile(r'\\b(Italy|Italian)\\b', re.IGNORECASE)\n",
    "for index, row in df.iterrows():\n",
    "    abstract = row['Abstract']\n",
    "    if isinstance(abstract, str):\n",
    "        tokens = wordpunct_tokenize(abstract)\n",
    "        bigrams_list = list(ngrams(tokens, 2))\n",
    "        re_search = [bigram for bigram in bigrams_list if pattern.search(bigram[0])]\n",
    "        bigrams.update(re_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Italy', ',')\t703\n",
      "('Italy', '.')\t282\n",
      "('Italy', 'and')\t274\n",
      "('Italy', 'is')\t94\n",
      "('Italy', '(')\t79\n",
      "('Italian', 'regions')\t77\n",
      "('Italy', 'has')\t69\n",
      "('Italian', 'Society')\t57\n",
      "('Italy', 'was')\t56\n",
      "('Italy', ')')\t54\n",
      "('Italian', 'population')\t34\n",
      "('Italian', 'government')\t33\n",
      "('Italian', 'National')\t28\n",
      "('Italian', 'Ministry')\t28\n",
      "('Italy', '),')\t27\n",
      "('Italy', 'in')\t26\n",
      "('Italian', 'Government')\t23\n",
      "('Italian', 'COVID')\t19\n",
      "('Italy', 'on')\t18\n",
      "('Italy', 'with')\t18\n"
     ]
    }
   ],
   "source": [
    "common_bigrams = bigrams.most_common(20)\n",
    "for i in common_bigrams:\n",
    "    print(str(i[0])+\"\\t\"+ str(i[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3\n",
    "\n",
    "## Test tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Italy',\n",
       " 'ITALY',\n",
       " 'ITALIAN',\n",
       " 'Italian',\n",
       " 'realm',\n",
       " 'right',\n",
       " 'italy',\n",
       " 'italian',\n",
       " 'ItaLian',\n",
       " 'iTalY']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'This is a Italy of the ITALY. ITALIAN Italian in the realm. That; right there is italy \"italian\" ItaLian iTalY.'\n",
    "tokens = wordpunct_tokenize(text)\n",
    "cleaned_tokens = [token for token in tokens if token.isalpha() and token.lower() not in stop_words]\n",
    "cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = Counter()\n",
    "pattern = re.compile(r'\\b(Italy|Italian)\\b', re.IGNORECASE)\n",
    "for index, row in df.iterrows():\n",
    "    abstract = row['Abstract']\n",
    "    if isinstance(abstract, str):\n",
    "        tokens = wordpunct_tokenize(abstract)\n",
    "        cleaned_tokens = [token for token in tokens if token.isalnum() and token.lower() not in stop_words]\n",
    "        bigrams_list = list(ngrams(cleaned_tokens, 2))\n",
    "        re_search = [bigram for bigram in bigrams_list if pattern.search(bigram[0])]\n",
    "        bigrams.update(re_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('Italy', 'Spain'), 177),\n",
       " (('Italian', 'regions'), 77),\n",
       " (('Italian', 'Society'), 57),\n",
       " (('Italy', 'one'), 56),\n",
       " (('Italy', 'first'), 52),\n",
       " (('Italy', 'Iran'), 49),\n",
       " (('Italian', 'population'), 34),\n",
       " (('Italian', 'government'), 33),\n",
       " (('Italy', 'Germany'), 29),\n",
       " (('Italy', 'France'), 29),\n",
       " (('Italian', 'National'), 28),\n",
       " (('Italy', 'China'), 28),\n",
       " (('Italian', 'Ministry'), 28),\n",
       " (('Italy', 'South'), 26),\n",
       " (('Italian', 'Government'), 23),\n",
       " (('Italy', 'USA'), 23),\n",
       " (('Italy', 'March'), 22),\n",
       " (('Italy', 'United'), 20),\n",
       " (('Italian', 'COVID'), 19),\n",
       " (('Italian', 'provinces'), 17)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_bigrams = bigrams.most_common(20)\n",
    "common_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('1557tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kodak Black - Codeine Dreaming &amp;gt;&amp;gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>give me some morphine, is there any more to do?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>just like nicotine,\\nheroin, morphine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Codeine hella calling my name 🙄</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Minus how much coke you youngins be on. Like y...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0            Kodak Black - Codeine Dreaming &gt;&gt;\n",
       "1    give me some morphine, is there any more to do?\n",
       "2              just like nicotine,\\nheroin, morphine\n",
       "3                    Codeine hella calling my name 🙄\n",
       "4  Minus how much coke you youngins be on. Like y..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Synset('gluttonize.v.01'), Synset('sip.v.01'), Synset('receive.v.12'), Synset('pub-crawl.v.01'), Synset('tuck_in.v.01'), Synset('hit.v.15'), Synset('wash_down.v.01'), Synset('ruminate.v.01'), Synset('pop.v.11'), Synset('guzzle.v.01'), Synset('fare.v.02'), Synset('dunk.v.03'), Synset('wine_and_dine.v.01'), Synset('eat_up.v.01'), Synset('sup.v.01'), Synset('eat_out.v.01'), Synset('toss_off.v.02'), Synset('crop.v.05'), Synset('eat.v.01'), Synset('raven.v.02'), Synset('lunch.v.01'), Synset('inject.v.04'), Synset('drop.v.17'), Synset('guggle.v.03'), Synset('eat.v.02'), Synset('tank.v.02'), Synset('use.v.02'), Synset('mess.v.01'), Synset('chain-smoke.v.01'), Synset('port.v.07'), Synset('slurp.v.01'), Synset('nibble.v.03'), Synset('fill_up.v.04'), Synset('mainline.v.01'), Synset('degust.v.01'), Synset('partake.v.03'), Synset('pitch_in.v.01'), Synset('board.v.03'), Synset('break_bread.v.01'), Synset('carry.v.33'), Synset('take_out.v.12'), Synset('swill.v.02'), Synset('picnic.v.01'), Synset('souse.v.03'), Synset('feed.v.06'), Synset('dine.v.01'), Synset('wolf.v.01'), Synset('gobble.v.01'), Synset('inhale.v.01'), Synset('puff.v.01'), Synset('drink.v.02'), Synset('swallow.v.01'), Synset('cannibalize.v.01'), Synset('skin_pop.v.01'), Synset('peck.v.02'), Synset('suck.v.01'), Synset('tipple.v.01'), Synset('claret.v.01'), Synset('lap.v.04'), Synset('victual.v.03'), Synset('devour.v.03'), Synset('browse.v.04'), Synset('devour.v.04'), Synset('forage.v.02'), Synset('wine.v.01'), Synset('cloy.v.02'), Synset('brunch.v.01'), Synset('suckle.v.01'), Synset('bolt.v.03'), Synset('drink.v.01'), Synset('gorge.v.01'), Synset('take_a_hit.v.01'), Synset('free-base.v.01'), Synset('garbage_down.v.01'), Synset('nosh.v.01'), Synset('drain_the_cup.v.01'), Synset('raven.v.04'), Synset('smoke.v.01'), Synset('take_in.v.14'), Synset('gulp.v.01'), Synset('drug.v.02'), Synset('pick_at.v.02'), Synset('sample.v.01'), Synset('feast.v.01'), Synset('drink.v.05'), Synset('satiate.v.01'), Synset('breakfast.v.01'), Synset('dope.v.01'), Synset('eat_in.v.01')}\n",
      "89\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def part01():\n",
    "    synset_consume = wn.synset('consume.v.02')\n",
    "    hyponyms = synset_consume.closure(lambda s: s.hyponyms())\n",
    "    return set(hyponyms)\n",
    "result = part01()\n",
    "print(result)\n",
    "print(len(result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bolt', 'gulp', 'souse', 'tope', 'fress', 'pall', 'guzzle', 'dunk', 'snack', 'whiff', 'pick', 'gurgle', 'inject', 'gobble', 'hit', 'englut', 'guggle', 'feast', 'drop', 'soak', 'dip', 'finish', 'swill', 'partake', 'forage', 'hold', 'stuff', 'victual', 'gormandise', 'breakfast', 'gluttonise', 'use', 'tipple', 'overeat', 'inhale', 'wine', 'gorge', 'dope', 'satiate', 'ruminate', 'port', 'habituate', 'suckle', 'ingurgitate', 'gluttonize', 'gourmandize', 'lunch', 'claret', 'lap', 'dine', 'drink', 'degust', 'cannibalise', 'inebriate', 'feed', 'imbibe', 'smoke', 'cannibalize', 'carry', 'sample', 'sup', 'board', 'graze', 'kill', 'browse', 'prey', 'drug', 'junket', 'nosh', 'swallow', 'slurp', 'piece', 'fuddle', 'pig', 'quaff', 'overindulge', 'brunch', 'suck', 'predate', 'free-base', 'binge', 'base', 'peck', 'eat', 'gormandize', 'banquet', 'chain-smoke', 'sip', 'engorge', 'fare', 'down', 'try', 'receive', 'cloy', 'mess', 'crop', 'picnic', 'glut', 'pub-crawl', 'overgorge', 'wolf', 'snort', 'tank', 'guttle', 'sate', 'mainline', 'pop', 'puff', 'bib', 'swig', 'booze', 'devour', 'fill', 'lick', 'taste', 'range', 'pasture', 'raven', 'replete', 'touch', 'nibble', 'consume'}\n",
      "122\n"
     ]
    }
   ],
   "source": [
    "def part02(synset_set):\n",
    "    lemmas = {lemma for synset in synset_set for lemma in synset.lemma_names()}\n",
    "    filtered_lemmas = {lemma for lemma in lemmas if '_' not in lemma}\n",
    "    return filtered_lemmas\n",
    "\n",
    "synsets = part01()\n",
    "lemmas = part02(synsets)\n",
    "print(lemmas)\n",
    "print(len(lemmas))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.7.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.10-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.9-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.3.0,>=8.2.2 (from spacy)\n",
      "  Downloading thinc-8.2.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.4.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer-0.12.5-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (4.66.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/codespace/.local/lib/python3.12/site-packages (from spacy) (2.32.3)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
      "  Downloading pydantic-2.9.1-py3-none-any.whl.metadata (146 kB)\n",
      "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.12/site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /home/codespace/.local/lib/python3.12/site-packages (from spacy) (73.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.12/site-packages (from spacy) (24.1)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.4.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /home/codespace/.local/lib/python3.12/site-packages (from spacy) (2.1.0)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading language_data-1.2.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.23.3 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading pydantic_core-2.23.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/codespace/.local/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.7.4)\n",
      "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.2.2->spacy)\n",
      "  Downloading blis-0.7.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.3.0,>=8.2.2->spacy)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting numpy>=1.19.0 (from spacy)\n",
      "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich>=10.11.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading rich-13.8.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading cloudpathlib-0.19.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading smart_open-7.0.4-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.12/site-packages (from jinja2->spacy) (2.1.5)\n",
      "Collecting marisa-trie>=0.7.7 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading marisa_trie-1.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.7 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/codespace/.local/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
      "Collecting wrapt (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading wrapt-1.16.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading spacy-3.7.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.7/31.7 MB\u001b[0m \u001b[31m90.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (46 kB)\n",
      "Downloading langcodes-3.4.0-py3-none-any.whl (182 kB)\n",
      "Downloading murmurhash-1.0.10-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n",
      "Downloading preshed-3.0.9-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (156 kB)\n",
      "Downloading pydantic-2.9.1-py3-none-any.whl (434 kB)\n",
      "Downloading pydantic_core-2.23.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.4.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (491 kB)\n",
      "Downloading thinc-8.2.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (865 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.0/865.0 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading typer-0.12.5-py3-none-any.whl (47 kB)\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading blis-0.7.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m85.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cloudpathlib-0.19.0-py3-none-any.whl (49 kB)\n",
      "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading language_data-1.2.0-py3-none-any.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m66.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rich-13.8.1-py3-none-any.whl (241 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading smart_open-7.0.4-py3-none-any.whl (61 kB)\n",
      "Downloading marisa_trie-1.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading wrapt-1.16.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: cymem, wrapt, wasabi, spacy-loggers, spacy-legacy, shellingham, pydantic-core, numpy, murmurhash, mdurl, marisa-trie, cloudpathlib, catalogue, annotated-types, srsly, smart-open, pydantic, preshed, markdown-it-py, language-data, blis, rich, langcodes, confection, typer, thinc, weasel, spacy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.1.0\n",
      "    Uninstalling numpy-2.1.0:\n",
      "      Successfully uninstalled numpy-2.1.0\n",
      "Successfully installed annotated-types-0.7.0 blis-0.7.11 catalogue-2.0.10 cloudpathlib-0.19.0 confection-0.1.5 cymem-2.0.8 langcodes-3.4.0 language-data-1.2.0 marisa-trie-1.2.0 markdown-it-py-3.0.0 mdurl-0.1.2 murmurhash-1.0.10 numpy-1.26.4 preshed-3.0.9 pydantic-2.9.1 pydantic-core-2.23.3 rich-13.8.1 shellingham-1.5.4 smart-open-7.0.4 spacy-3.7.6 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.2.5 typer-0.12.5 wasabi-1.1.3 weasel-0.4.1 wrapt-1.16.0\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stopwords\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Counter\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/spacy/__init__.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Dict, Iterable, Union\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# set library-specific custom warning handling before doing anything else\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m setup_default_warnings\n\u001b[1;32m      8\u001b[0m setup_default_warnings()  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# These are imported as part of the API\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/spacy/errors.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Literal\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mErrorsWithCodes\u001b[39;00m(\u001b[38;5;28mtype\u001b[39m):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, code):\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/spacy/compat.py:39\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcatalogue\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _importlib_metadata \u001b[38;5;28;01mas\u001b[39;00m importlib_metadata  \u001b[38;5;66;03m# type: ignore[no-redef]    # noqa: F401\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthinc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Optimizer  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     41\u001b[0m pickle \u001b[38;5;241m=\u001b[39m pickle\n\u001b[1;32m     42\u001b[0m copy_reg \u001b[38;5;241m=\u001b[39m copy_reg\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/thinc/api.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m     CupyOps,\n\u001b[1;32m      3\u001b[0m     MPSOps,\n\u001b[1;32m      4\u001b[0m     NumpyOps,\n\u001b[1;32m      5\u001b[0m     Ops,\n\u001b[1;32m      6\u001b[0m     get_current_ops,\n\u001b[1;32m      7\u001b[0m     get_ops,\n\u001b[1;32m      8\u001b[0m     set_current_ops,\n\u001b[1;32m      9\u001b[0m     set_gpu_allocator,\n\u001b[1;32m     10\u001b[0m     use_ops,\n\u001b[1;32m     11\u001b[0m     use_pytorch_for_gpu_memory,\n\u001b[1;32m     12\u001b[0m     use_tensorflow_for_gpu_memory,\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m enable_mxnet, enable_tensorflow, has_cupy\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config, ConfigValidationError, registry\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/thinc/backends/__init__.py:17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_cupy_allocators\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cupy_pytorch_allocator, cupy_tensorflow_allocator\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_server\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ParamServer\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcupy_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CupyOps\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmps_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MPSOps\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NumpyOps\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/thinc/backends/cupy_ops.py:16\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      7\u001b[0m     is_cupy_array,\n\u001b[1;32m      8\u001b[0m     is_mxnet_gpu_array,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     torch2xp,\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _custom_kernels\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NumpyOps\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Ops\n\u001b[1;32m     20\u001b[0m \u001b[38;5;129m@registry\u001b[39m\u001b[38;5;241m.\u001b[39mops(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCupyOps\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCupyOps\u001b[39;00m(Ops):\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/thinc/backends/numpy_ops.pyx:1\u001b[0m, in \u001b[0;36minit thinc.backends.numpy_ops\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import string\n",
    "\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "def part03(texts, consume_terms):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    punctuation = set(string.punctuation)\n",
    "    bigram_counter = Counter()\n",
    "    for text in texts:\n",
    "        doc = nlp(text)\n",
    "        tokens = [\n",
    "            token.lemma_.lower() for token in doc\n",
    "            if token.lemma_.lower() not in stop_words and token.lemma_ not in punctuation and not token.is_punct\n",
    "        ]\n",
    "        for i in range(len(tokens) - 1):\n",
    "            first_word = tokens[i]\n",
    "            second_word = tokens[i + 1]\n",
    "            if first_word in consume_terms:\n",
    "                bigram_counter[(first_word, second_word)] += 1\n",
    "    return bigram_counter\n",
    "\n",
    "# Example usage:\n",
    "# Load the tweets from the CSV file\n",
    "df = pd.read_csv('1157tweets.csv')\n",
    "\n",
    "# Get the list of consume terms by calling part01 and part02\n",
    "consume_terms = part02(part01())\n",
    "\n",
    "# Call part03 to process the tweets and find relevant bigrams\n",
    "res = part03(df['text'], consume_terms)\n",
    "\n",
    "# Print the most common bigrams\n",
    "print(res.most_common(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Usage:   \n",
      "  pip uninstall [options] <package> ...\n",
      "  pip uninstall [options] -r <requirements file> ...\n",
      "\n",
      "-r option requires 1 argument\n",
      "\n",
      "Usage:   \n",
      "  pip install [options] <requirement specifier> [package-index-options] ...\n",
      "  pip install [options] -r <requirements file> [package-index-options] ...\n",
      "  pip install [options] [-e] <vcs project url> ...\n",
      "  pip install [options] [-e] <local project path> ...\n",
      "  pip install [options] <archive url/path> ...\n",
      "\n",
      "no such option: --yes\n"
     ]
    }
   ],
   "source": [
    "! --yes -r\n",
    "! --yes -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spacy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 2\u001b[0m     nlp \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcli\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m download\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spacy' is not defined"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except OSError:\n",
    "    from spacy.cli import download\n",
    "    download(\"en_core_web_sm\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/spacy/__init__.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Dict, Iterable, Union\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# set library-specific custom warning handling before doing anything else\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m setup_default_warnings\n\u001b[1;32m      8\u001b[0m setup_default_warnings()  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# These are imported as part of the API\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/spacy/errors.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Literal\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mErrorsWithCodes\u001b[39;00m(\u001b[38;5;28mtype\u001b[39m):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, code):\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/spacy/compat.py:39\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcatalogue\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _importlib_metadata \u001b[38;5;28;01mas\u001b[39;00m importlib_metadata  \u001b[38;5;66;03m# type: ignore[no-redef]    # noqa: F401\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthinc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Optimizer  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     41\u001b[0m pickle \u001b[38;5;241m=\u001b[39m pickle\n\u001b[1;32m     42\u001b[0m copy_reg \u001b[38;5;241m=\u001b[39m copy_reg\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/thinc/api.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m     CupyOps,\n\u001b[1;32m      3\u001b[0m     MPSOps,\n\u001b[1;32m      4\u001b[0m     NumpyOps,\n\u001b[1;32m      5\u001b[0m     Ops,\n\u001b[1;32m      6\u001b[0m     get_current_ops,\n\u001b[1;32m      7\u001b[0m     get_ops,\n\u001b[1;32m      8\u001b[0m     set_current_ops,\n\u001b[1;32m      9\u001b[0m     set_gpu_allocator,\n\u001b[1;32m     10\u001b[0m     use_ops,\n\u001b[1;32m     11\u001b[0m     use_pytorch_for_gpu_memory,\n\u001b[1;32m     12\u001b[0m     use_tensorflow_for_gpu_memory,\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m enable_mxnet, enable_tensorflow, has_cupy\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config, ConfigValidationError, registry\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/thinc/backends/__init__.py:17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_cupy_allocators\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cupy_pytorch_allocator, cupy_tensorflow_allocator\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_server\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ParamServer\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcupy_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CupyOps\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmps_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MPSOps\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NumpyOps\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/thinc/backends/cupy_ops.py:16\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      7\u001b[0m     is_cupy_array,\n\u001b[1;32m      8\u001b[0m     is_mxnet_gpu_array,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     torch2xp,\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _custom_kernels\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NumpyOps\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Ops\n\u001b[1;32m     20\u001b[0m \u001b[38;5;129m@registry\u001b[39m\u001b[38;5;241m.\u001b[39mops(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCupyOps\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCupyOps\u001b[39;00m(Ops):\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/thinc/backends/numpy_ops.pyx:1\u001b[0m, in \u001b[0;36minit thinc.backends.numpy_ops\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: health, coronavirus, use, research, cost, human, public, application, invention, telemedicine\n",
      "Topic 2: registered, study, research, obtained, irb, guidelines, relevant, id, competing, reporting\n",
      "Topic 3: cov, sars, protein, virus, mers, binding, coronavirus, spike, cell, human\n",
      "Topic 4: covid, 19, pandemic, health, care, medical, patients, healthcare, risk, social\n",
      "Topic 5: sars, 19, cov, covid, coronavirus, disease, respiratory, infection, 2019, treatment\n",
      "Topic 6: study, 19, covid, research, patients, 95, amp, data, ci, trial\n",
      "Topic 7: 19, covid, health, cases, pandemic, 2020, countries, data, measures, public\n",
      "Topic 8: patients, 19, covid, disease, severe, risk, clinical, studies, mortality, infection\n",
      "Topic 9: patients, 19, covid, cases, results, sars, cov, positive, clinical, pcr\n",
      "Topic 10: mask, masks, transmission, virus, use, n95, viral, respiratory, wearing, protective\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/joblib/externals/loky/backend/fork_exec.py:38: DeprecationWarning: This process (pid=7272) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
      "  pid = os.fork()\n",
      "/home/codespace/.local/lib/python3.12/site-packages/joblib/externals/loky/backend/fork_exec.py:38: DeprecationWarning: This process (pid=7272) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
      "  pid = os.fork()\n",
      "/home/codespace/.local/lib/python3.12/site-packages/joblib/externals/loky/backend/fork_exec.py:38: DeprecationWarning: This process (pid=7272) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
      "  pid = os.fork()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from langdetect import detect\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import pyLDAvis\n",
    "# Step 1: Load data (Assume you load your dataset into a DataFrame)\n",
    "data = pd.read_excel('/workspaces/BMIG60303-assignments/All_Articles_Excel_Dec2019July2020.xlsx')\n",
    "df = data[['Abstract']].dropna()  # Filter out missing abstracts\n",
    "\n",
    "# Step 2: Ensure all abstracts are strings and clean data\n",
    "df['Abstract'] = df['Abstract'].astype(str)  # Convert all entries to strings\n",
    "\n",
    "# Step 3: Detect English abstracts\n",
    "def is_english(text):\n",
    "    try:\n",
    "        return detect(text) == 'en'\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "df['English'] = df['Abstract'].apply(is_english)\n",
    "\n",
    "# Step 4: Extract 5000 random abstracts\n",
    "random_abstracts_df = df[df['English']].sample(n=5000, random_state=0)  # Only English abstracts\n",
    "\n",
    "# Step 5: Vectorize abstracts using CountVectorizer\n",
    "vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "X = vectorizer.fit_transform(random_abstracts_df['Abstract'])\n",
    "\n",
    "# Step 6: Fit LDA model with 10 topics\n",
    "lda = LatentDirichletAllocation(n_components=10, random_state=0)\n",
    "lda.fit(X)\n",
    "\n",
    "# Step 7: Function to display topics\n",
    "def get_topic_words(lda_model, feature_names, n_top_words=10):\n",
    "    topics = {}\n",
    "    for topic_idx, topic in enumerate(lda_model.components_):\n",
    "        topic_words = [feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "        topics[f\"Topic {topic_idx+1}\"] = topic_words\n",
    "    return topics\n",
    "\n",
    "# Get the top words for each topic\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "topics = get_topic_words(lda, feature_names)\n",
    "for topic, words in topics.items():\n",
    "    print(f'{topic}: {\", \".join(words)}')\n",
    "\n",
    "# Step 8: Visualize with PyLDAVis (Updated for newer versions)\n",
    "\n",
    "# Extract necessary values for pyLDAvis\n",
    "vocab = vectorizer.get_feature_names_out()  # Vocabulary\n",
    "term_frequency = X.sum(axis=0).A1  # Term frequencies\n",
    "doc_lengths = X.sum(axis=1).A1  # Document lengths (needed for visualization)\n",
    "\n",
    "# Prepare the visualization\n",
    "lda_vis = pyLDAvis.prepare(\n",
    "    topic_term_dists=lda.components_,\n",
    "    doc_topic_dists=lda.transform(X),\n",
    "    doc_lengths=doc_lengths,\n",
    "    vocab=vocab,\n",
    "    term_frequency=term_frequency\n",
    ")\n",
    "\n",
    "# Save visualization as HTML\n",
    "pyLDAvis.save_html(lda_vis, 'lda_part1.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA with 10 Topics (Custom Stopwords & Tokenizer):\n",
      "\n",
      "Topic 1:\n",
      "hybrid parallel education. videos face-to-face advances sports directions strategies. describing\n",
      "\n",
      "Topic 2:\n",
      "mpro anal. quercetin voice beliefs shared wind opening promise remove\n",
      "\n",
      "Topic 3:\n",
      ", health pandemic covid-19 public social ; response care crisis\n",
      "\n",
      "Topic 4:\n",
      "migrants youth updates correction humanity emerge respect following gates article\n",
      "\n",
      "Topic 5:\n",
      "pad gi peer crime australia − australian eastern danger manual\n",
      "\n",
      "Topic 6:\n",
      ", ) ( % : covid-19 patients study ; registered\n",
      "\n",
      "Topic 7:\n",
      "copyright users abstract print [ ] download abstracts holder emailed\n",
      "\n",
      "Topic 8:\n",
      "• t-cell apps ssa hyperglycemia interview log dangerous dealing comply\n",
      "\n",
      "Topic 9:\n",
      "conjunctival charged hace2 div built instruments mpro publishing charge pandemia\n",
      "\n",
      "Topic 10:\n",
      ", covid-19 patients ) ( disease . respiratory care pandemic\n",
      "\n",
      "LDA with 5 Topics (Custom Stopwords & Tokenizer):\n",
      "\n",
      "Topic 1:\n",
      "br div layer < neurosurgical table manual exercise top complete\n",
      "\n",
      "Topic 2:\n",
      ", sars-cov-2 ) ( virus protein ace2 drugs viral human\n",
      "\n",
      "Topic 3:\n",
      ", ) ( covid-19 : health registered study ; research\n",
      "\n",
      "Topic 4:\n",
      "dentists dental dentistry ecg leukaemia qtc gc fractures updates supplemental\n",
      "\n",
      "Topic 5:\n",
      ", ) ( patients % covid-19 : disease respiratory severe\n",
      "\n",
      "LDA with 15 Topics (Custom Stopwords & Tokenizer):\n",
      "\n",
      "Topic 1:\n",
      "drastic danger n't focusing completely face . , response. curb\n",
      "\n",
      "Topic 2:\n",
      "3clpro tb epitope humoral camels crime s-protein peptides subjective kcal/mol\n",
      "\n",
      "Topic 3:\n",
      "country trinidad tobago response safeguard up-to-date ministry continues 039 #\n",
      "\n",
      "Topic 4:\n",
      "hcov-oc43 voice supplemental repeatedly ease scientists surge restrictions second infections\n",
      "\n",
      "Topic 5:\n",
      "nrf2 pad proinflammatory il6 colloidal ms dementia metagenomic sot coated\n",
      "\n",
      "Topic 6:\n",
      ", ) ( % : covid-19 patients sars-cov-2 study ;\n",
      "\n",
      "Topic 7:\n",
      "copyright users abstract may print posted download abstracts emailed listserv\n",
      "\n",
      "Topic 8:\n",
      "substances antigens tnf pt gut businesses microbiota chains tumor dietary\n",
      "\n",
      "Topic 9:\n",
      "posts ppi updates vital symptoms spread remove filters 4.7 searches\n",
      "\n",
      "Topic 10:\n",
      ", covid-19 health pandemic care ) ( . patients disease\n",
      "\n",
      "Topic 11:\n",
      "de la en el que cats los para las por\n",
      "\n",
      "Topic 12:\n",
      "ais hemodialysis gc pm dietary correction sport urgency entering visible\n",
      "\n",
      "Topic 13:\n",
      "tweets twitter br moral jobs extract mscs labs comment hip\n",
      "\n",
      "Topic 14:\n",
      "• john dangerous generated bleeding pandemic causes lobes assessing german\n",
      "\n",
      "Topic 15:\n",
      "phc paper conjunctival accessed hcp municipal interdisciplinary minimally undergo dependence\n",
      "\n",
      "LDA with 10 Topics (Fewer Features):\n",
      "\n",
      "Topic 1:\n",
      "anxiety depression sleep psychological quot mental stress & perceived distress\n",
      "\n",
      "Topic 2:\n",
      ", ) ( % patients covid-19 : . disease ;\n",
      "\n",
      "Topic 3:\n",
      "transplantation transplant bcg organ recipients paper via top published vaccination\n",
      "\n",
      "Topic 4:\n",
      "ultrasound ecmo common exercise top complete . lung workforce kidney\n",
      "\n",
      "Topic 5:\n",
      "masks tracheostomy mask n95 face respirators airway see u.s. describes\n",
      "\n",
      "Topic 6:\n",
      "invention detection , translation antibody method machine parts kit preparation\n",
      "\n",
      "Topic 7:\n",
      ", covid-19 health pandemic ) ( care . ; :\n",
      "\n",
      "Topic 8:\n",
      "copyright users abstract may posted [ ] original abstracts property\n",
      "\n",
      "Topic 9:\n",
      ", sars-cov-2 ) ( virus covid-19 viral respiratory human .\n",
      "\n",
      "Topic 10:\n",
      "registered , study research id prospective reporting trial followed appropriate\n",
      "\n",
      "LDA with 10 Topics (No Stopwords):\n",
      "\n",
      "Topic 1:\n",
      "ms consists originating identifies partners faster scale. complement peptides 3.6\n",
      "\n",
      "Topic 2:\n",
      "ggo camels eyes anal. abstract qrt-pcr sot ari beliefs febrile\n",
      "\n",
      "Topic 3:\n",
      "pedv rbd 3clpro americans codon thereof cleavage humoral tmprss2 furin\n",
      "\n",
      "Topic 4:\n",
      "season 3cl stated interview author with quercetin pro catalytic canadian\n",
      "\n",
      "Topic 5:\n",
      "de la en el y que moral para los las\n",
      "\n",
      "Topic 6:\n",
      "registered been have any study research id prospective reporting followed\n",
      "\n",
      "Topic 7:\n",
      "decoction parts qi thereof clearing composition facial oil heat relates\n",
      "\n",
      "Topic 8:\n",
      "sport br canine hyperglycemia influences herbs updates − dangerous generated\n",
      "\n",
      "Topic 9:\n",
      "country trinidad tobago response safeguard up-to-date ministry continues face 039\n",
      "\n",
      "Topic 10:\n",
      ", to in ) ( a covid-19 with for is\n",
      "\n",
      "LDA with 10 Topics (Default Stopwords and Tokenizer):\n",
      "\n",
      "Topic 1:\n",
      "opioid inequality crime figure ap substituted text professor conspiracy reduces\n",
      "\n",
      "Topic 2:\n",
      "la en el que para los las por se como\n",
      "\n",
      "Topic 3:\n",
      "arthroplasty br americas hip gustatory dysfunctions seizures interview 254 fractures\n",
      "\n",
      "Topic 4:\n",
      "pet corticosteroid epicentre ivermectin charged rd requested humanity emerge respect\n",
      "\n",
      "Topic 5:\n",
      "abstract copyright users property holder download applies print abstracts emailed\n",
      "\n",
      "Topic 6:\n",
      "health covid 19 pandemic care public medical healthcare social coronavirus\n",
      "\n",
      "Topic 7:\n",
      "patients 19 covid disease respiratory cov sars severe coronavirus 2019\n",
      "\n",
      "Topic 8:\n",
      "palliative doi corrected trainees article version lancet online ophthalmology skills\n",
      "\n",
      "Topic 9:\n",
      "study registered cov sars research 19 covid data obtained trial\n",
      "\n",
      "Topic 10:\n",
      "n95 respirators cloth radix paper accessed webinar decontamination respirator rhizoma\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from langdetect import detect\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "\n",
    "# Helper function to detect if the text is in English\n",
    "def is_english(text):\n",
    "    try:\n",
    "        return detect(text) == 'en'\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# Function to fit and display topics from an LDA model\n",
    "def fit_and_print_lda(X, n_topics, vectorizer, n_top_words=10):\n",
    "    lda = LatentDirichletAllocation(n_components=n_topics, random_state=0)\n",
    "    lda.fit(X)\n",
    "\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        print(f\"\\nTopic {topic_idx + 1}:\")\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "\n",
    "# Load dataset (replace with actual dataset path)\n",
    "df = pd.read_excel('/workspaces/BMIG60303-assignments/All_Articles_Excel_Dec2019July2020.xlsx')\n",
    "\n",
    "# Step 1: Filter out non-English abstracts\n",
    "df['English'] = df['Abstract'].apply(is_english)\n",
    "df = df[df['English']]  # Filter to include only English abstracts\n",
    "\n",
    "# Step 2: Extract 5000 random abstracts\n",
    "random_abstracts_df = df.sample(n=5000, random_state=0)\n",
    "\n",
    "# Define the custom tokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "# Define custom stopwords (can be expanded)\n",
    "custom_stopwords = set(stopwords.words('english')).union({'covid', 'coronavirus', 'sars', 'cov', '19', 'doi'})\n",
    "custom_stopwords_list = list(custom_stopwords)  # Convert to list for TfidfVectorizer\n",
    "\n",
    "# Step 3: Experimenting with different configurations\n",
    "\n",
    "# 1. Use TfidfVectorizer with custom tokenization and stopwords\n",
    "tfidf_vectorizer_custom_stopwords = TfidfVectorizer(\n",
    "    max_df=0.95,\n",
    "    min_df=5,\n",
    "    stop_words=custom_stopwords_list,  # Now the stopwords are passed as a list\n",
    "    tokenizer=tokenizer.tokenize, \n",
    "    max_features=5000\n",
    ")\n",
    "\n",
    "# Fit and transform the abstracts\n",
    "X_tfidf_custom_stopwords = tfidf_vectorizer_custom_stopwords.fit_transform(random_abstracts_df['Abstract'])\n",
    "\n",
    "# Apply LDA with 10 topics and print results\n",
    "print(\"LDA with 10 Topics (Custom Stopwords & Tokenizer):\")\n",
    "fit_and_print_lda(X_tfidf_custom_stopwords, n_topics=10, vectorizer=tfidf_vectorizer_custom_stopwords)\n",
    "\n",
    "# 2. Experiment with fewer topics (e.g., 5 topics)\n",
    "print(\"\\nLDA with 5 Topics (Custom Stopwords & Tokenizer):\")\n",
    "fit_and_print_lda(X_tfidf_custom_stopwords, n_topics=5, vectorizer=tfidf_vectorizer_custom_stopwords)\n",
    "\n",
    "# 3. Experiment with more topics (e.g., 15 topics)\n",
    "print(\"\\nLDA with 15 Topics (Custom Stopwords & Tokenizer):\")\n",
    "fit_and_print_lda(X_tfidf_custom_stopwords, n_topics=15, vectorizer=tfidf_vectorizer_custom_stopwords)\n",
    "\n",
    "# 4. Experiment with different number of features\n",
    "tfidf_vectorizer_fewer_features = TfidfVectorizer(\n",
    "    max_df=0.95,\n",
    "    min_df=5,\n",
    "    stop_words=custom_stopwords_list,\n",
    "    tokenizer=tokenizer.tokenize,\n",
    "    max_features=2000  # Reduce the number of features\n",
    ")\n",
    "\n",
    "X_tfidf_fewer_features = tfidf_vectorizer_fewer_features.fit_transform(random_abstracts_df['Abstract'])\n",
    "\n",
    "# LDA with 10 topics and fewer features\n",
    "print(\"\\nLDA with 10 Topics (Fewer Features):\")\n",
    "fit_and_print_lda(X_tfidf_fewer_features, n_topics=10, vectorizer=tfidf_vectorizer_fewer_features)\n",
    "\n",
    "# 5. Experiment without stopwords\n",
    "tfidf_vectorizer_no_stopwords = TfidfVectorizer(\n",
    "    max_df=0.95,\n",
    "    min_df=5,\n",
    "    tokenizer=tokenizer.tokenize,\n",
    "    max_features=5000\n",
    ")\n",
    "\n",
    "X_tfidf_no_stopwords = tfidf_vectorizer_no_stopwords.fit_transform(random_abstracts_df['Abstract'])\n",
    "\n",
    "# LDA without stopwords and 10 topics\n",
    "print(\"\\nLDA with 10 Topics (No Stopwords):\")\n",
    "fit_and_print_lda(X_tfidf_no_stopwords, n_topics=10, vectorizer=tfidf_vectorizer_no_stopwords)\n",
    "\n",
    "# Step 4: Summary of the changes in topics\n",
    "\n",
    "# 1. Default stopwords and default tokenization\n",
    "print(\"\\nLDA with 10 Topics (Default Stopwords and Tokenizer):\")\n",
    "vectorizer_default = TfidfVectorizer(max_df=0.95, min_df=5, stop_words='english', max_features=5000)\n",
    "X_default = vectorizer_default.fit_transform(random_abstracts_df['Abstract'])\n",
    "fit_and_print_lda(X_default, n_topics=10, vectorizer=vectorizer_default)\n",
    "\n",
    "# 2. Adding a different tokenizer and adjusting stopwords\n",
    "# Already done above with custom tokenization and custom stopwords\n",
    "\n",
    "# 3. Modifying the number of topics\n",
    "# Done with n_components as 5, 10, and 15 in experiments\n",
    "\n",
    "# You can add more experiments to see the impact of features, topics, or vectorization settings\n",
    "\n",
    "# Final Conclusions can be drawn based on the outputs, themes, and coherence of the topics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from random import shuffle\n",
    "import os\n",
    "import re\n",
    "import tarfile\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np  # Keras takes care of most of this but it likes to see Numpy arrays\n",
    "from keras.preprocessing import sequence    # A helper module to handle padding input\n",
    "from keras.models import Sequential         # The base keras Neural Network model\n",
    "from keras.layers import Dense, Dropout, Activation   # The layer objects we will pile into the model\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# pull in a dataframe of ade texts and labels\n",
    "ade_df = pd.read_parquet(\"hf://datasets/ade-benchmark-corpus/ade_corpus_v2/Ade_corpus_v2_classification/train-00000-of-00001.parquet\")\n",
    "    \n",
    "# Google pre-trained vectors, available here:\n",
    "# https://code.google.com/archive/p/word2vec/\n",
    "googlevecs = 'GoogleNews-vectors-negative300.bin.gz'\n",
    "word_vectors = KeyedVectors.load_word2vec_format(googlevecs, binary=True, limit=200000)\n",
    "\n",
    "def pre_process_data(filepath):\n",
    "    \"\"\"\n",
    "    This is dependent on your training data source but we will try to generalize it as best as possible.\n",
    "    \"\"\"\n",
    "    positive_path = os.path.join(filepath, 'pos')\n",
    "    negative_path = os.path.join(filepath, 'neg')\n",
    "\n",
    "    pos_label = 1\n",
    "    neg_label = 0\n",
    "\n",
    "    dataset = []\n",
    "\n",
    "    for filename in glob.glob(os.path.join(positive_path, '*.txt')):\n",
    "        with open(filename, 'r') as f:\n",
    "            dataset.append((pos_label, f.read()))\n",
    "\n",
    "    for filename in glob.glob(os.path.join(negative_path, '*.txt')):\n",
    "        with open(filename, 'r') as f:\n",
    "            dataset.append((neg_label, f.read()))\n",
    "\n",
    "    shuffle(dataset)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def tokenize_and_vectorize(dataset):\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "    vectorized_data = []\n",
    "    expected = []\n",
    "\n",
    "    for sample in dataset:\n",
    "        tokens = tokenizer.tokenize(sample[1])\n",
    "\n",
    "        sample_vecs = []\n",
    "        for token in tokens:\n",
    "            try:\n",
    "                sample_vecs.append(word_vectors[token])\n",
    "            except KeyError:\n",
    "#                print(f\"not found: {token}\")\n",
    "                pass\n",
    "             \n",
    "        vectorized_data.append(sample_vecs)\n",
    "    return vectorized_data\n",
    "\n",
    "\n",
    "\n",
    "def collect_expected(dataset):\n",
    "    \"\"\" Peel of the target values from the dataset \"\"\"\n",
    "    expected = []\n",
    "    for sample in dataset:\n",
    "        expected.append(sample[0])\n",
    "    return expected\n",
    "\n",
    "\"\"\"\n",
    "Following the previous example, the Dataset format is a list of tuples,\n",
    "with label as the first element and text as the second element.\n",
    "Here we re-shape the ADE dataframe to fit this format.\n",
    "\"\"\"\n",
    "dataset = [(x[2], x[1]) for x in ade_df.itertuples()]\n",
    "\n",
    "\"\"\"\n",
    "Note: as discussed in class on Oct 17, many terms relevant to identifying adverse drug events,\n",
    "such a drug names and the terms for particular symptoms/problems, may not be available in the\n",
    "word2vec embeddings generated from Google news.\n",
    "\n",
    "One solution would be to use another set of pre-computed vectors.\n",
    "If there's not a good option for this type of text, we also might look into building our own.\n",
    "\"\"\"\n",
    "vectorized_data = tokenize_and_vectorize(dataset)\n",
    "\n",
    "expected = collect_expected(dataset)\n",
    "\n",
    "split_point = int(len(vectorized_data) * .8)\n",
    "\n",
    "x_train = vectorized_data[:split_point]\n",
    "y_train = expected[:split_point]\n",
    "x_test = vectorized_data[split_point:]\n",
    "y_test = expected[split_point:]\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Note: as discussed in class on Oct 17, 400 may be too large as a mex len for the ADE corpus.\n",
    "The result would be many zero-vectors diluting the meaning of each document.\n",
    "A solution wouldbe to pick a smaller value of maxlen.\n",
    "\"\"\"\n",
    "maxlen = 100\n",
    "batch_size = 32         # How many samples to show the net before backpropogating the error and updating the weights\n",
    "embedding_dims = 300    # Length of the token vectors we will create for passing into the Convnet\n",
    "filters = 250           # Number of filters we will train\n",
    "kernel_size = 3         # The width of the filters, actual filters will each be a matrix of weights of size: embedding_dims x kernel_size or 50 x 3 in our case\n",
    "hidden_dims = 250       # Number of neurons in the plain feed forward net at the end of the chain\n",
    "epochs = 2              # Number of times we will pass the entire training dataset through the network\n",
    "\n",
    "\n",
    "def pad_trunc(data, maxlen):\n",
    "    \"\"\" For a given dataset pad with zero vectors or truncate to maxlen \"\"\"\n",
    "    new_data = []\n",
    "\n",
    "    # Create a vector of 0's the length of our word vectors\n",
    "    zero_vector = []\n",
    "    for _ in range(len(data[0][0])):\n",
    "        zero_vector.append(0.0)\n",
    "\n",
    "    for sample in data:\n",
    "\n",
    "        if len(sample) > maxlen:\n",
    "            temp = sample[:maxlen]\n",
    "        elif len(sample) < maxlen:\n",
    "            temp = sample\n",
    "            additional_elems = maxlen - len(sample)\n",
    "            for _ in range(additional_elems):\n",
    "                temp.append(zero_vector)\n",
    "        else:\n",
    "            temp = sample\n",
    "        new_data.append(temp)\n",
    "    return new_data\n",
    "\n",
    "\n",
    "x_train = pad_trunc(x_train, maxlen)\n",
    "x_test = pad_trunc(x_test, maxlen)\n",
    "\n",
    "x_train = np.reshape(x_train, (len(x_train), maxlen, embedding_dims))\n",
    "y_train = np.array(y_train)\n",
    "x_test = np.reshape(x_test, (len(x_test), maxlen, embedding_dims))\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# we add a Convolution1D, which will learn filters\n",
    "# word group filters of size filter_length:\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1,\n",
    "                 input_shape=(maxlen, embedding_dims)))\n",
    "\n",
    "# we use max pooling:\n",
    "model.add(GlobalMaxPooling1D())\n",
    "# We add a vanilla hidden layer:\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "def makevec(s):\n",
    "    vl = tokenize_and_vectorize([(1, s)])\n",
    "    tvl = pad_trunc(vl, maxlen)\n",
    "    tv = np.reshape(tvl, (len(tvl), maxlen, embedding_dims))\n",
    "    return tv\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# try running some predictions\n",
    "\n",
    "model.predict(makevec(\"Patient reports rash and itching after starting amoxicillin.\"))\n",
    "\n",
    "model.predict(makevec(\"Blood pressure stabilized with current lisinopril regimen.\"))\n",
    "\n",
    "model.predict(makevec(\"The old Dune movie was terrible and barely worth watching.\"))\n",
    "\n",
    "# Look out for these ones-- as we saw on 10/17, the key words are not found in our vectors!\n",
    "model.predict(makevec(\"Patient reports akathesia after starting abilify\"))\n",
    "\n",
    "model.predict(makevec(\"Patient reports after starting\"))\n",
    "\n",
    "model.predict(makevec(\"Patient reports movement disorder after starting abilify\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "explore document length distribution\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "c = Counter([len(text) for (label, text) in dataset])\n",
    "\n",
    "# Extracting keys and values from the Counter\n",
    "labels, values = zip(*c.items())\n",
    "\n",
    "# Create the bar chart\n",
    "plt.bar(labels, values)\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Number of documents of each length')\n",
    "plt.xlabel('Num tokens')\n",
    "plt.ylabel('Num documents')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting huggingface_hub\n",
      "  Downloading huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.12/site-packages (from huggingface_hub) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/codespace/.local/lib/python3.12/site-packages (from huggingface_hub) (2024.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/codespace/.local/lib/python3.12/site-packages (from huggingface_hub) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/codespace/.local/lib/python3.12/site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in /home/codespace/.local/lib/python3.12/site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/codespace/.python/current/lib/python3.12/site-packages (from huggingface_hub) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/codespace/.local/lib/python3.12/site-packages (from huggingface_hub) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests->huggingface_hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests->huggingface_hub) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests->huggingface_hub) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests->huggingface_hub) (2024.7.4)\n",
      "Downloading huggingface_hub-0.26.2-py3-none-any.whl (447 kB)\n",
      "Installing collected packages: huggingface_hub\n",
      "Successfully installed huggingface_hub-0.26.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
